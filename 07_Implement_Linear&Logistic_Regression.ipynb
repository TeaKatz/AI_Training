{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Linear Regression and Logistic Regression\n",
    "<img src=\"pics/bridge-2.jpg\" width=\"800\" height=\"400\">\n",
    "In this article you're going to learn about Linear Regression, Logistic Regression and Gradient Descent algorithm which is a essential component of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "1. How does it work?\n",
    "2. Gradient Descent\n",
    "3. Learning Rate\n",
    "3. Implement Linear Regression on quantitative data\n",
    "4. Implement Logistic Regression on qualitative data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How does it work?\n",
    "Unlike KNN which needs to store whole training samples in order to predict output of unseen samples, Linear Regression and Logistic Regression do not store a single point of training sample but instead they create a linear function to approximate the training set. Below are brief procedures of how to train both of them using **Gradient Descent** (actually there is another method called **Least Square Error** but we're not going to talk about it in this course).\n",
    "\n",
    "<img src=\"pics/linear_regression-2.png\" width=\"1000\">\n",
    "\n",
    "```\n",
    "1. Initial the model with random parameters\n",
    "2. for _ in range(epochs):\n",
    "3.    Predict output for training samples from the linear function\n",
    "4.    Calculate cost from prediction outputs and training labels\n",
    "5.    Update parameters by Gradient descent\n",
    "```\n",
    "\n",
    "<img src=\"pics/linear_regression_animation.gif\">\n",
    "\n",
    "There are several differents between Linear Regression and Logistic Regression.\n",
    "1. Linear Regression outputs **continuous quantity**, Logistic Regression outputs **probability**.\n",
    "2. Linear Regression uses **Mean Squared Error** or **Mean Absolute Error** as cost function, Logistic Regression uses **Binary Cross Entropy** as cost function.\n",
    "\n",
    "In short, the obvious different between the two is that Linear Regression is for **quantitative data** and another is for **qualitative data**. the following will show how both of them work in greater detail.\n",
    "\n",
    "### 1.1. Linear Regression\n",
    "Linear Regression's equation is a simple linear function that map from N independent variables to a dependent variable.\n",
    "- Simple Linear Regression\n",
    "#### $$\\hat{y} = w_0 + wx$$\n",
    "- Multiple Linear Regression\n",
    "#### $$\\hat{y} = w_0 + w_1x_1 + w_2x_2 + w_nx_n$$\n",
    "- Polynomial Linear Regression\n",
    "#### $$\\hat{y} = w_0 + w_1x_1 + w_2x_2 + w_3x_1x_2 + w_4x_1^2 + w_5x_2^2 + w_nx_n$$\n",
    "\n",
    "<img src=\"pics/linear_regression-3.png\" width=\"800\" height=\"200\">\n",
    "\n",
    "In order for Gradient Descent algorithm to be able to train the model we need a cost function for optimizer to minimize it.\n",
    "- Mean Squared Error (MSE)\n",
    "#### $$MSE = \\frac{1}{n}\\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "<img src=\"pics/MSE.png\">\n",
    "\n",
    "- Mean Absolute Error (MAE)\n",
    "#### $$MAE = \\frac{1}{n}\\sum_{i=1}^n{|y_i - \\hat{y}_i|}$$\n",
    "\n",
    "<img src=\"pics/MAE.png\">\n",
    "\n",
    "### 1.2. Logistic Regression\n",
    "Logistic Regression is a linear equation mapping any N independent variables into one dependent variable that is a **qualitative data**.\n",
    "- Simple Logistic Regression\n",
    "#### $$\\hat{y} = \\frac{1}{1 + e^{-z}},\\quad z = w_0 + wx$$\n",
    "- Multiple Logistic Regression\n",
    "#### $$\\hat{y} = \\frac{1}{1 + e^{-z}},\\quad z = w_0 + w_1x_1 + w_2x_2 + w_nx_n$$\n",
    "- Polynomial Logistic Regression\n",
    "#### $$\\hat{y} = \\frac{1}{1 + e^{-z}},\\quad z = w_0 + w_1x_1 + w_2x_2 + w_3x_1x_2 + w_4x_1^2 + w_5x_2^2 + w_nx_n$$\n",
    "\n",
    "<img src=\"pics/logistic_regression-3.png\" width=\"1400\" height=\"200\">\n",
    "\n",
    "Since output of Logistic Regression is probability so it need different type of cost function.\n",
    "- Binary Cross Entropy (BCE)\n",
    "####  $$BCE = \\frac{1}{n}\\sum_{i=1}^n{(-y_i*log(\\hat{y}_i) - (1 - y_i)*log(1 - \\hat{y}_i))}$$\n",
    "\n",
    "<img src=\"pics/BCE.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent\n",
    "Gradient descent is an optimization algorithm used in machine learning to update model's parameters to the optimal point by minimizing some function by moving iteratively in the steepest descent direction as defined by the negative of the gradient.\n",
    "- Gradient Descent Equation\n",
    "#### $$W_{new} = W_{old} - \\alpha*\\frac{\\partial J(W)}{\\partial W}$$\n",
    "When  \n",
    "$J(W)$: Cost function  \n",
    "$W$: Model parameters  \n",
    "$\\alpha$: Learning rate\n",
    "\n",
    "<img src=\"pics/GradientDescent.png\" width=\"500\">\n",
    "\n",
    "From the figure above the model with initialized parameters start at the **inital point**, then in each epoch the model finds which direction to update each parameter to move closer to the **optimal point** by calculating partial derivative of the cost function with respect to each parameter, and update its parameters by subtraction current value of each parameter with the derivative times learning rate.\n",
    "\n",
    "- Calculating derivative for LinearRegression\n",
    "#### $$J(w_0, w_1) = \\frac{1}{n}\\sum_{i=1}^n{(y_i - \\hat{y}_i)^2},\\quad \\hat{y} = w_0 + w_1x$$\n",
    "#### $$\\frac{\\partial J(w_0, w_1)}{\\partial w_0} = \\frac{1}{n}\\sum_{i=1}^n{\\frac{\\partial J(w_0, w_1)}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial w_0}} = \\frac{1}{n}\\sum_{i=1}^n{-2(y_i - \\hat{y}_i) \\times 1}$$\n",
    "#### $$\\frac{\\partial J(w_0, w_1)}{\\partial w_1} = \\frac{1}{n}\\sum_{i=1}^n{\\frac{\\partial J(w_0, w_1)}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial w_1}} = \\frac{1}{n}\\sum_{i=1}^n{-2(y_i - \\hat{y}_i) \\times x}$$\n",
    "\n",
    "- Calculating derivative for LogisticRegression\n",
    "#### $$J(w_0, w_1) = \\frac{1}{n}\\sum_{i=1}^n{(-y_i*log(\\hat{y}_i) - (1 - y_i)*log(1 - \\hat{y}_i))},\\quad \\hat{y} = \\frac{1}{1 + e^{-z}},\\quad z = w_0 + wx$$\n",
    "#### $$\\frac{\\partial J(w_0, w_1)}{\\partial w_0} = \\frac{1}{n}\\sum_{i=1}^n{\\frac{\\partial J(w_0, w_1)}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial z} \\times \\frac{\\partial z}{\\partial w_0}} = \\frac{1}{n}\\sum_{i=1}^n{\\left(-\\frac{y_i}{\\hat{y}_i} + \\frac{1-y_i}{1-\\hat{y}_i}\\right) \\times \\hat{y}_i(1 - \\hat{y}_i) \\times 1}$$\n",
    "#### $$\\frac{\\partial J(w_0, w_1)}{\\partial w_1} = \\frac{1}{n}\\sum_{i=1}^n{\\frac{\\partial J(w_0, w_1)}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial z} \\times \\frac{\\partial z}{\\partial w_1}} = \\frac{1}{n}\\sum_{i=1}^n{\\left(-\\frac{y_i}{\\hat{y}_i} + \\frac{1-y_i}{1-\\hat{y}_i}\\right) \\times \\hat{y}_i(1 - \\hat{y}_i) \\times x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Learning Rate\n",
    "Learning rate is one of hyperparameters which used to determine how much to update parameters, too small learning rate leads to slow in training time and too big learning rate leads to oscillation or divergence.\n",
    "\n",
    "<img src=\"pics/LearningRate.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement Linear regression on quantitative data\n",
    "This time we're going to use Linear Regression to predict house price instead of KNN, let see if Linear Regression can do any better than the KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "house_data = pd.read_csv(\"./datasets/housedata/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>313000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1340</td>\n",
       "      <td>7912</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1340</td>\n",
       "      <td>0</td>\n",
       "      <td>Shoreline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2384000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3650</td>\n",
       "      <td>9050</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3370</td>\n",
       "      <td>280</td>\n",
       "      <td>Seattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>342000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1930</td>\n",
       "      <td>11947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1930</td>\n",
       "      <td>0</td>\n",
       "      <td>Kent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>420000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2000</td>\n",
       "      <td>8030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>Bellevue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>550000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1940</td>\n",
       "      <td>10500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1140</td>\n",
       "      <td>800</td>\n",
       "      <td>Redmond</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  \\\n",
       "0   313000.0       3.0       1.50         1340      7912     1.5           0   \n",
       "1  2384000.0       5.0       2.50         3650      9050     2.0           0   \n",
       "2   342000.0       3.0       2.00         1930     11947     1.0           0   \n",
       "3   420000.0       3.0       2.25         2000      8030     1.0           0   \n",
       "4   550000.0       4.0       2.50         1940     10500     1.0           0   \n",
       "\n",
       "   view  condition  sqft_above  sqft_basement       city  \n",
       "0     0          3        1340              0  Shoreline  \n",
       "1     4          5        3370            280    Seattle  \n",
       "2     0          4        1930              0       Kent  \n",
       "3     0          4        1000           1000   Bellevue  \n",
       "4     0          4        1140            800    Redmond  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove columns\n",
    "data = house_data.drop(columns=[\"date\", \"yr_built\", \"yr_renovated\", \"street\", \"statezip\", \"country\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>view_0</th>\n",
       "      <th>...</th>\n",
       "      <th>city_SeaTac</th>\n",
       "      <th>city_Seattle</th>\n",
       "      <th>city_Shoreline</th>\n",
       "      <th>city_Skykomish</th>\n",
       "      <th>city_Snoqualmie</th>\n",
       "      <th>city_Snoqualmie Pass</th>\n",
       "      <th>city_Tukwila</th>\n",
       "      <th>city_Vashon</th>\n",
       "      <th>city_Woodinville</th>\n",
       "      <th>city_Yarrow Point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>313000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1340</td>\n",
       "      <td>7912</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1340</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2384000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3650</td>\n",
       "      <td>9050</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3370</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>342000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1930</td>\n",
       "      <td>11947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1930</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>420000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2000</td>\n",
       "      <td>8030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>550000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1940</td>\n",
       "      <td>10500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1140</td>\n",
       "      <td>800</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  \\\n",
       "0   313000.0       3.0       1.50         1340      7912     1.5           0   \n",
       "1  2384000.0       5.0       2.50         3650      9050     2.0           0   \n",
       "2   342000.0       3.0       2.00         1930     11947     1.0           0   \n",
       "3   420000.0       3.0       2.25         2000      8030     1.0           0   \n",
       "4   550000.0       4.0       2.50         1940     10500     1.0           0   \n",
       "\n",
       "   sqft_above  sqft_basement  view_0  ...  city_SeaTac  city_Seattle  \\\n",
       "0        1340              0       1  ...            0             0   \n",
       "1        3370            280       0  ...            0             1   \n",
       "2        1930              0       1  ...            0             0   \n",
       "3        1000           1000       1  ...            0             0   \n",
       "4        1140            800       1  ...            0             0   \n",
       "\n",
       "   city_Shoreline  city_Skykomish  city_Snoqualmie  city_Snoqualmie Pass  \\\n",
       "0               1               0                0                     0   \n",
       "1               0               0                0                     0   \n",
       "2               0               0                0                     0   \n",
       "3               0               0                0                     0   \n",
       "4               0               0                0                     0   \n",
       "\n",
       "   city_Tukwila  city_Vashon  city_Woodinville  city_Yarrow Point  \n",
       "0             0            0                 0                  0  \n",
       "1             0            0                 0                  0  \n",
       "2             0            0                 0                  0  \n",
       "3             0            0                 0                  0  \n",
       "4             0            0                 0                  0  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding categorical columns\n",
    "categorical_cols = [\"view\", \"condition\", \"city\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    city_encoded = pd.get_dummies(data[col])\n",
    "    city_encoded.columns = [col + \"_\" + str(_col) for _col in city_encoded.columns]\n",
    "    data = pd.concat([data.drop(columns=col), city_encoded], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_x: (4600, 62)\n",
      "data_y: (4600,)\n"
     ]
    }
   ],
   "source": [
    "# Seperate prediction/feature\n",
    "data_x = data.drop(columns=\"price\")\n",
    "data_y = data.price\n",
    "print(f\"data_x: {data_x.shape}\")\n",
    "print(f\"data_y: {data_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (3680, 62)\n",
      "test_x: (920, 62)\n",
      "train_y: (3680,)\n",
      "test_y: (920,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split train/test\n",
    "# Group y into bins\n",
    "bins = np.linspace(0, 1500000, 10)\n",
    "y_binned = np.digitize(data_y, bins)\n",
    "plt.hist(y_binned)\n",
    "\n",
    "# Split with stratify\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, random_state=42, shuffle=True, stratify=y_binned)\n",
    "print(f\"train_x: {train_x.shape}\")\n",
    "print(f\"test_x: {test_x.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\")\n",
    "print(f\"test_y: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>view_0</th>\n",
       "      <th>view_1</th>\n",
       "      <th>...</th>\n",
       "      <th>city_SeaTac</th>\n",
       "      <th>city_Seattle</th>\n",
       "      <th>city_Shoreline</th>\n",
       "      <th>city_Skykomish</th>\n",
       "      <th>city_Snoqualmie</th>\n",
       "      <th>city_Snoqualmie Pass</th>\n",
       "      <th>city_Tukwila</th>\n",
       "      <th>city_Vashon</th>\n",
       "      <th>city_Woodinville</th>\n",
       "      <th>city_Yarrow Point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "      <td>3.680000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>-1.073115e-16</td>\n",
       "      <td>3.722867e-17</td>\n",
       "      <td>1.099513e-16</td>\n",
       "      <td>6.174107e-17</td>\n",
       "      <td>-1.614349e-16</td>\n",
       "      <td>-5.539349e-16</td>\n",
       "      <td>1.601979e-17</td>\n",
       "      <td>-1.844539e-16</td>\n",
       "      <td>-1.740606e-16</td>\n",
       "      <td>4.121929e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.204728e-16</td>\n",
       "      <td>-1.450531e-16</td>\n",
       "      <td>-2.180019e-16</td>\n",
       "      <td>-2.796129e-16</td>\n",
       "      <td>-4.686921e-16</td>\n",
       "      <td>5.629574e-16</td>\n",
       "      <td>4.476492e-16</td>\n",
       "      <td>1.993303e-15</td>\n",
       "      <td>1.267102e-16</td>\n",
       "      <td>-2.331525e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "      <td>1.000136e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-3.732891e+00</td>\n",
       "      <td>-2.747640e+00</td>\n",
       "      <td>-1.827296e+00</td>\n",
       "      <td>-4.225101e-01</td>\n",
       "      <td>-9.559976e-01</td>\n",
       "      <td>-8.102192e-02</td>\n",
       "      <td>-1.686014e+00</td>\n",
       "      <td>-6.692456e-01</td>\n",
       "      <td>-2.990977e+00</td>\n",
       "      <td>-1.197204e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.597200e-02</td>\n",
       "      <td>-7.272373e-01</td>\n",
       "      <td>-1.627861e-01</td>\n",
       "      <td>-2.331896e-02</td>\n",
       "      <td>-1.287423e-01</td>\n",
       "      <td>-1.648675e-02</td>\n",
       "      <td>-8.270396e-02</td>\n",
       "      <td>-8.102192e-02</td>\n",
       "      <td>-1.645366e-01</td>\n",
       "      <td>-2.331896e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-4.375834e-01</td>\n",
       "      <td>-5.225785e-01</td>\n",
       "      <td>-6.964308e-01</td>\n",
       "      <td>-2.898747e-01</td>\n",
       "      <td>-9.559976e-01</td>\n",
       "      <td>-8.102192e-02</td>\n",
       "      <td>-7.330443e-01</td>\n",
       "      <td>-6.692456e-01</td>\n",
       "      <td>3.343389e-01</td>\n",
       "      <td>-1.197204e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.597200e-02</td>\n",
       "      <td>-7.272373e-01</td>\n",
       "      <td>-1.627861e-01</td>\n",
       "      <td>-2.331896e-02</td>\n",
       "      <td>-1.287423e-01</td>\n",
       "      <td>-1.648675e-02</td>\n",
       "      <td>-8.270396e-02</td>\n",
       "      <td>-8.102192e-02</td>\n",
       "      <td>-1.645366e-01</td>\n",
       "      <td>-2.331896e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>-4.375834e-01</td>\n",
       "      <td>1.131534e-01</td>\n",
       "      <td>-1.673104e-01</td>\n",
       "      <td>-2.092657e-01</td>\n",
       "      <td>-3.286163e-02</td>\n",
       "      <td>-8.102192e-02</td>\n",
       "      <td>-2.798025e-01</td>\n",
       "      <td>-6.692456e-01</td>\n",
       "      <td>3.343389e-01</td>\n",
       "      <td>-1.197204e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.597200e-02</td>\n",
       "      <td>-7.272373e-01</td>\n",
       "      <td>-1.627861e-01</td>\n",
       "      <td>-2.331896e-02</td>\n",
       "      <td>-1.287423e-01</td>\n",
       "      <td>-1.648675e-02</td>\n",
       "      <td>-8.270396e-02</td>\n",
       "      <td>-8.102192e-02</td>\n",
       "      <td>-1.645366e-01</td>\n",
       "      <td>-2.331896e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>6.608524e-01</td>\n",
       "      <td>4.310193e-01</td>\n",
       "      <td>4.966839e-01</td>\n",
       "      <td>-1.094699e-01</td>\n",
       "      <td>8.902743e-01</td>\n",
       "      <td>-8.102192e-02</td>\n",
       "      <td>5.453298e-01</td>\n",
       "      <td>6.239690e-01</td>\n",
       "      <td>3.343389e-01</td>\n",
       "      <td>-1.197204e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.597200e-02</td>\n",
       "      <td>1.375067e+00</td>\n",
       "      <td>-1.627861e-01</td>\n",
       "      <td>-2.331896e-02</td>\n",
       "      <td>-1.287423e-01</td>\n",
       "      <td>-1.648675e-02</td>\n",
       "      <td>-8.270396e-02</td>\n",
       "      <td>-8.102192e-02</td>\n",
       "      <td>-1.645366e-01</td>\n",
       "      <td>-2.331896e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>6.153032e+00</td>\n",
       "      <td>7.424070e+00</td>\n",
       "      <td>1.183646e+01</td>\n",
       "      <td>1.905516e+01</td>\n",
       "      <td>3.659682e+00</td>\n",
       "      <td>1.234234e+01</td>\n",
       "      <td>8.819897e+00</td>\n",
       "      <td>9.719578e+00</td>\n",
       "      <td>3.343389e-01</td>\n",
       "      <td>8.352798e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.163169e+01</td>\n",
       "      <td>1.375067e+00</td>\n",
       "      <td>6.143032e+00</td>\n",
       "      <td>4.288356e+01</td>\n",
       "      <td>7.767453e+00</td>\n",
       "      <td>6.065476e+01</td>\n",
       "      <td>1.209132e+01</td>\n",
       "      <td>1.234234e+01</td>\n",
       "      <td>6.077676e+00</td>\n",
       "      <td>4.288356e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           bedrooms     bathrooms   sqft_living      sqft_lot        floors  \\\n",
       "count  3.680000e+03  3.680000e+03  3.680000e+03  3.680000e+03  3.680000e+03   \n",
       "mean  -1.073115e-16  3.722867e-17  1.099513e-16  6.174107e-17 -1.614349e-16   \n",
       "std    1.000136e+00  1.000136e+00  1.000136e+00  1.000136e+00  1.000136e+00   \n",
       "min   -3.732891e+00 -2.747640e+00 -1.827296e+00 -4.225101e-01 -9.559976e-01   \n",
       "25%   -4.375834e-01 -5.225785e-01 -6.964308e-01 -2.898747e-01 -9.559976e-01   \n",
       "50%   -4.375834e-01  1.131534e-01 -1.673104e-01 -2.092657e-01 -3.286163e-02   \n",
       "75%    6.608524e-01  4.310193e-01  4.966839e-01 -1.094699e-01  8.902743e-01   \n",
       "max    6.153032e+00  7.424070e+00  1.183646e+01  1.905516e+01  3.659682e+00   \n",
       "\n",
       "         waterfront    sqft_above  sqft_basement        view_0        view_1  \\\n",
       "count  3.680000e+03  3.680000e+03   3.680000e+03  3.680000e+03  3.680000e+03   \n",
       "mean  -5.539349e-16  1.601979e-17  -1.844539e-16 -1.740606e-16  4.121929e-16   \n",
       "std    1.000136e+00  1.000136e+00   1.000136e+00  1.000136e+00  1.000136e+00   \n",
       "min   -8.102192e-02 -1.686014e+00  -6.692456e-01 -2.990977e+00 -1.197204e-01   \n",
       "25%   -8.102192e-02 -7.330443e-01  -6.692456e-01  3.343389e-01 -1.197204e-01   \n",
       "50%   -8.102192e-02 -2.798025e-01  -6.692456e-01  3.343389e-01 -1.197204e-01   \n",
       "75%   -8.102192e-02  5.453298e-01   6.239690e-01  3.343389e-01 -1.197204e-01   \n",
       "max    1.234234e+01  8.819897e+00   9.719578e+00  3.343389e-01  8.352798e+00   \n",
       "\n",
       "       ...   city_SeaTac  city_Seattle  city_Shoreline  city_Skykomish  \\\n",
       "count  ...  3.680000e+03  3.680000e+03    3.680000e+03    3.680000e+03   \n",
       "mean   ...  1.204728e-16 -1.450531e-16   -2.180019e-16   -2.796129e-16   \n",
       "std    ...  1.000136e+00  1.000136e+00    1.000136e+00    1.000136e+00   \n",
       "min    ... -8.597200e-02 -7.272373e-01   -1.627861e-01   -2.331896e-02   \n",
       "25%    ... -8.597200e-02 -7.272373e-01   -1.627861e-01   -2.331896e-02   \n",
       "50%    ... -8.597200e-02 -7.272373e-01   -1.627861e-01   -2.331896e-02   \n",
       "75%    ... -8.597200e-02  1.375067e+00   -1.627861e-01   -2.331896e-02   \n",
       "max    ...  1.163169e+01  1.375067e+00    6.143032e+00    4.288356e+01   \n",
       "\n",
       "       city_Snoqualmie  city_Snoqualmie Pass  city_Tukwila   city_Vashon  \\\n",
       "count     3.680000e+03          3.680000e+03  3.680000e+03  3.680000e+03   \n",
       "mean     -4.686921e-16          5.629574e-16  4.476492e-16  1.993303e-15   \n",
       "std       1.000136e+00          1.000136e+00  1.000136e+00  1.000136e+00   \n",
       "min      -1.287423e-01         -1.648675e-02 -8.270396e-02 -8.102192e-02   \n",
       "25%      -1.287423e-01         -1.648675e-02 -8.270396e-02 -8.102192e-02   \n",
       "50%      -1.287423e-01         -1.648675e-02 -8.270396e-02 -8.102192e-02   \n",
       "75%      -1.287423e-01         -1.648675e-02 -8.270396e-02 -8.102192e-02   \n",
       "max       7.767453e+00          6.065476e+01  1.209132e+01  1.234234e+01   \n",
       "\n",
       "       city_Woodinville  city_Yarrow Point  \n",
       "count      3.680000e+03       3.680000e+03  \n",
       "mean       1.267102e-16      -2.331525e-16  \n",
       "std        1.000136e+00       1.000136e+00  \n",
       "min       -1.645366e-01      -2.331896e-02  \n",
       "25%       -1.645366e-01      -2.331896e-02  \n",
       "50%       -1.645366e-01      -2.331896e-02  \n",
       "75%       -1.645366e-01      -2.331896e-02  \n",
       "max        6.077676e+00       4.288356e+01  \n",
       "\n",
       "[8 rows x 62 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize all features to be in range [0, 1] for training set\n",
    "scaler_x = StandardScaler()\n",
    "\n",
    "scaler_x.fit(train_x)\n",
    "\n",
    "train_x_scaled = scaler_x.transform(train_x)\n",
    "train_x_scaled = pd.DataFrame(train_x_scaled, columns=train_x.columns)\n",
    "train_x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>view_0</th>\n",
       "      <th>view_1</th>\n",
       "      <th>...</th>\n",
       "      <th>city_SeaTac</th>\n",
       "      <th>city_Seattle</th>\n",
       "      <th>city_Shoreline</th>\n",
       "      <th>city_Skykomish</th>\n",
       "      <th>city_Snoqualmie</th>\n",
       "      <th>city_Snoqualmie Pass</th>\n",
       "      <th>city_Tukwila</th>\n",
       "      <th>city_Vashon</th>\n",
       "      <th>city_Woodinville</th>\n",
       "      <th>city_Yarrow Point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>9.200000e+02</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>920.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.013730</td>\n",
       "      <td>-0.001209</td>\n",
       "      <td>0.041928</td>\n",
       "      <td>0.048555</td>\n",
       "      <td>-0.052930</td>\n",
       "      <td>0.040511</td>\n",
       "      <td>0.037795</td>\n",
       "      <td>0.017009</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>0.036837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060499</td>\n",
       "      <td>-0.041703</td>\n",
       "      <td>0.029130</td>\n",
       "      <td>0.023319</td>\n",
       "      <td>-0.034331</td>\n",
       "      <td>-1.648675e-02</td>\n",
       "      <td>-0.029773</td>\n",
       "      <td>-0.013504</td>\n",
       "      <td>-0.042406</td>\n",
       "      <td>0.069957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.991444</td>\n",
       "      <td>0.982609</td>\n",
       "      <td>0.995879</td>\n",
       "      <td>1.397089</td>\n",
       "      <td>0.967554</td>\n",
       "      <td>1.223398</td>\n",
       "      <td>1.009287</td>\n",
       "      <td>1.001793</td>\n",
       "      <td>0.988420</td>\n",
       "      <td>1.141640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546042</td>\n",
       "      <td>0.986053</td>\n",
       "      <td>1.083805</td>\n",
       "      <td>1.414598</td>\n",
       "      <td>0.858706</td>\n",
       "      <td>2.291080e-16</td>\n",
       "      <td>0.801420</td>\n",
       "      <td>0.913867</td>\n",
       "      <td>0.865020</td>\n",
       "      <td>1.999455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-3.732891</td>\n",
       "      <td>-2.747640</td>\n",
       "      <td>-1.671672</td>\n",
       "      <td>-0.421203</td>\n",
       "      <td>-0.955998</td>\n",
       "      <td>-0.081022</td>\n",
       "      <td>-1.511690</td>\n",
       "      <td>-0.669246</td>\n",
       "      <td>-2.990977</td>\n",
       "      <td>-0.119720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085972</td>\n",
       "      <td>-0.727237</td>\n",
       "      <td>-0.162786</td>\n",
       "      <td>-0.023319</td>\n",
       "      <td>-0.128742</td>\n",
       "      <td>-1.648675e-02</td>\n",
       "      <td>-0.082704</td>\n",
       "      <td>-0.081022</td>\n",
       "      <td>-0.164537</td>\n",
       "      <td>-0.023319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-0.437583</td>\n",
       "      <td>-0.522579</td>\n",
       "      <td>-0.665306</td>\n",
       "      <td>-0.277712</td>\n",
       "      <td>-0.955998</td>\n",
       "      <td>-0.081022</td>\n",
       "      <td>-0.735950</td>\n",
       "      <td>-0.669246</td>\n",
       "      <td>0.334339</td>\n",
       "      <td>-0.119720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085972</td>\n",
       "      <td>-0.727237</td>\n",
       "      <td>-0.162786</td>\n",
       "      <td>-0.023319</td>\n",
       "      <td>-0.128742</td>\n",
       "      <td>-1.648675e-02</td>\n",
       "      <td>-0.082704</td>\n",
       "      <td>-0.081022</td>\n",
       "      <td>-0.164537</td>\n",
       "      <td>-0.023319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>-0.437583</td>\n",
       "      <td>0.113153</td>\n",
       "      <td>-0.130998</td>\n",
       "      <td>-0.206088</td>\n",
       "      <td>-0.032862</td>\n",
       "      <td>-0.081022</td>\n",
       "      <td>-0.221695</td>\n",
       "      <td>-0.669246</td>\n",
       "      <td>0.334339</td>\n",
       "      <td>-0.119720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085972</td>\n",
       "      <td>-0.727237</td>\n",
       "      <td>-0.162786</td>\n",
       "      <td>-0.023319</td>\n",
       "      <td>-0.128742</td>\n",
       "      <td>-1.648675e-02</td>\n",
       "      <td>-0.082704</td>\n",
       "      <td>-0.081022</td>\n",
       "      <td>-0.164537</td>\n",
       "      <td>-0.023319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.660852</td>\n",
       "      <td>0.431019</td>\n",
       "      <td>0.548558</td>\n",
       "      <td>-0.100241</td>\n",
       "      <td>0.890274</td>\n",
       "      <td>-0.081022</td>\n",
       "      <td>0.615059</td>\n",
       "      <td>0.688630</td>\n",
       "      <td>0.334339</td>\n",
       "      <td>-0.119720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085972</td>\n",
       "      <td>1.375067</td>\n",
       "      <td>-0.162786</td>\n",
       "      <td>-0.023319</td>\n",
       "      <td>-0.128742</td>\n",
       "      <td>-1.648675e-02</td>\n",
       "      <td>-0.082704</td>\n",
       "      <td>-0.081022</td>\n",
       "      <td>-0.164537</td>\n",
       "      <td>-0.023319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>3.956160</td>\n",
       "      <td>4.245411</td>\n",
       "      <td>5.030520</td>\n",
       "      <td>32.221854</td>\n",
       "      <td>2.736546</td>\n",
       "      <td>12.342339</td>\n",
       "      <td>4.078291</td>\n",
       "      <td>4.029434</td>\n",
       "      <td>0.334339</td>\n",
       "      <td>8.352798</td>\n",
       "      <td>...</td>\n",
       "      <td>11.631694</td>\n",
       "      <td>1.375067</td>\n",
       "      <td>6.143032</td>\n",
       "      <td>42.883563</td>\n",
       "      <td>7.767453</td>\n",
       "      <td>-1.648675e-02</td>\n",
       "      <td>12.091319</td>\n",
       "      <td>12.342339</td>\n",
       "      <td>6.077676</td>\n",
       "      <td>42.883563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         bedrooms   bathrooms  sqft_living    sqft_lot      floors  \\\n",
       "count  920.000000  920.000000   920.000000  920.000000  920.000000   \n",
       "mean     0.013730   -0.001209     0.041928    0.048555   -0.052930   \n",
       "std      0.991444    0.982609     0.995879    1.397089    0.967554   \n",
       "min     -3.732891   -2.747640    -1.671672   -0.421203   -0.955998   \n",
       "25%     -0.437583   -0.522579    -0.665306   -0.277712   -0.955998   \n",
       "50%     -0.437583    0.113153    -0.130998   -0.206088   -0.032862   \n",
       "75%      0.660852    0.431019     0.548558   -0.100241    0.890274   \n",
       "max      3.956160    4.245411     5.030520   32.221854    2.736546   \n",
       "\n",
       "       waterfront  sqft_above  sqft_basement      view_0      view_1  ...  \\\n",
       "count  920.000000  920.000000     920.000000  920.000000  920.000000  ...   \n",
       "mean     0.040511    0.037795       0.017009    0.009036    0.036837  ...   \n",
       "std      1.223398    1.009287       1.001793    0.988420    1.141640  ...   \n",
       "min     -0.081022   -1.511690      -0.669246   -2.990977   -0.119720  ...   \n",
       "25%     -0.081022   -0.735950      -0.669246    0.334339   -0.119720  ...   \n",
       "50%     -0.081022   -0.221695      -0.669246    0.334339   -0.119720  ...   \n",
       "75%     -0.081022    0.615059       0.688630    0.334339   -0.119720  ...   \n",
       "max     12.342339    4.078291       4.029434    0.334339    8.352798  ...   \n",
       "\n",
       "       city_SeaTac  city_Seattle  city_Shoreline  city_Skykomish  \\\n",
       "count   920.000000    920.000000      920.000000      920.000000   \n",
       "mean     -0.060499     -0.041703        0.029130        0.023319   \n",
       "std       0.546042      0.986053        1.083805        1.414598   \n",
       "min      -0.085972     -0.727237       -0.162786       -0.023319   \n",
       "25%      -0.085972     -0.727237       -0.162786       -0.023319   \n",
       "50%      -0.085972     -0.727237       -0.162786       -0.023319   \n",
       "75%      -0.085972      1.375067       -0.162786       -0.023319   \n",
       "max      11.631694      1.375067        6.143032       42.883563   \n",
       "\n",
       "       city_Snoqualmie  city_Snoqualmie Pass  city_Tukwila  city_Vashon  \\\n",
       "count       920.000000          9.200000e+02    920.000000   920.000000   \n",
       "mean         -0.034331         -1.648675e-02     -0.029773    -0.013504   \n",
       "std           0.858706          2.291080e-16      0.801420     0.913867   \n",
       "min          -0.128742         -1.648675e-02     -0.082704    -0.081022   \n",
       "25%          -0.128742         -1.648675e-02     -0.082704    -0.081022   \n",
       "50%          -0.128742         -1.648675e-02     -0.082704    -0.081022   \n",
       "75%          -0.128742         -1.648675e-02     -0.082704    -0.081022   \n",
       "max           7.767453         -1.648675e-02     12.091319    12.342339   \n",
       "\n",
       "       city_Woodinville  city_Yarrow Point  \n",
       "count        920.000000         920.000000  \n",
       "mean          -0.042406           0.069957  \n",
       "std            0.865020           1.999455  \n",
       "min           -0.164537          -0.023319  \n",
       "25%           -0.164537          -0.023319  \n",
       "50%           -0.164537          -0.023319  \n",
       "75%           -0.164537          -0.023319  \n",
       "max            6.077676          42.883563  \n",
       "\n",
       "[8 rows x 62 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize all features to be in range [0, 1] for test set\n",
    "test_x_scaled = scaler_x.transform(test_x)\n",
    "test_x_scaled = pd.DataFrame(test_x_scaled, columns=test_x.columns)\n",
    "test_x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert type to numpy array\n",
    "train_x_scaled = train_x_scaled.to_numpy()\n",
    "train_x, train_y = train_x.to_numpy(), train_y.to_numpy()\n",
    "\n",
    "test_x_scaled = test_x_scaled.to_numpy()\n",
    "test_x, test_y = test_x.to_numpy(), test_y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Prepare model\n",
    "Let's implement the following Linear Regression with features size of 4 in matrix form.\n",
    "#### $$\\hat{y} = w_0 + w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4$$\n",
    "\n",
    "We can write it in matrix form as the following.\n",
    "#### $$\\hat{Y} = X^TW =\n",
    "\\begin{bmatrix}\n",
    "1 & x_1 & x_2 & x_3 & x_4\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_3 \\\\\n",
    "w_4 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "However, inputs to the model can be more than one sample at a time as shown below.\n",
    "#### $$\\hat{y}_1 = w_0 + w_1x_{11} + w_2x_{12} + w_3x_{13} + w_4x_{14}$$\n",
    "#### $$\\hat{y}_2 = w_0 + w_1x_{21} + w_2x_{22} + w_3x_{23} + w_4x_{24}$$\n",
    "#### $$\\hat{y}_3 = w_0 + w_1x_{31} + w_2x_{32} + w_3x_{33} + w_4x_{34}$$\n",
    "\n",
    "And we can write it in matrix form as following.\n",
    "#### $$\\hat{Y} = X^TW =\n",
    "\\begin{bmatrix}\n",
    "1 & x_{11} & x_{12} & x_{13} & x_{14} \\\\\n",
    "1 & x_{21} & x_{22} & x_{23} & x_{24} \\\\\n",
    "1 & x_{31} & x_{32} & x_{33} & x_{34} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_3 \\\\\n",
    "w_4 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now you can see that, what you have to do to implement Linear Regression in matrix form are:\n",
    "1. Initial weight vector with size equal to feature numbers of input + 1.\n",
    "2. Concatenate vector of one to every batch of input with the vector size equal to batch size.\n",
    "3. Do matrix multiplication between inputs and the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        # w shape: (feature_size, 1)\n",
    "        self.w = self.add_weight(name=\"W\",\n",
    "                                 shape=(input_shape[-1] + 1, 1),\n",
    "                                 initializer=tf.random_normal_initializer(),\n",
    "                                 dtype=\"float32\")\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'w': self.w})\n",
    "        return config\n",
    "    \n",
    "    def call(self, inp):\n",
    "        \"\"\"\n",
    "        inp shape: (batch_size, feature_size)\n",
    "        out shape: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Cast input type\n",
    "        inp = tf.cast(inp, tf.float32)\n",
    "        \n",
    "        # Concatenate one vector to input\n",
    "        bias = tf.ones(shape=(tf.shape(inp)[0], 1), dtype=inp.dtype)\n",
    "        inp = tf.concat([bias, inp], axis=1)\n",
    "        \n",
    "        # Linear operation\n",
    "        out = tf.matmul(inp, self.w)\n",
    "        return out\n",
    "    \n",
    "class LinearRegression(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer = LinearLayer()\n",
    "        \n",
    "    def call(self, inp):\n",
    "        out = self.linear_layer(inp)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (1000, 1)\n",
      "Parameters:\n",
      "[[0.15693742]\n",
      " [0.04215328]]\n"
     ]
    }
   ],
   "source": [
    "dummy_x = np.arange(1000).astype(np.float32).reshape(-1, 1)\n",
    "dummy_y = (dummy_x * 230 + 1).astype(np.float32).reshape(-1, 1) + np.random.normal(0, 1, size=(1000, 1)).astype(np.float32) * 0\n",
    "\n",
    "regression_model = LinearRegression()\n",
    "\n",
    "y_pred = regression_model(dummy_x)\n",
    "print(f\"Output shape: {y_pred.shape}\")\n",
    "print(f\"Parameters:\\n{regression_model.trainable_variables[0].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Fit and evaluate model\n",
    "There are two important steps for training model.\n",
    "1. Forward propagation\n",
    "2. Calculate derivatives of loss function with respect to each of model parameter (calculate gradient)\n",
    "3. Update model parameters using the prior derivatives\n",
    "\n",
    "Actually, Tensorflow provides both **High-level API** and **Low-level API** for training model so that you don't have to write code to calculate derivatives and update parameters by yourself, but to understand what going on under the api let's try to implement a code to calculate derivatives for the following simple linear model with 2 parameters by yourself.\n",
    "- Simple linear model with 2 parameters\n",
    "#### $$\\hat{y} = w_0 + w_1x_1$$\n",
    "- Loss function\n",
    "#### $$J(w_0, w_1) = \\frac{1}{n}\\sum_{i=0}^n{(y_i - \\hat{y}_i)^2}$$\n",
    "- Equations for calculating derivative of the loss function with respect to each parameter\n",
    "#### $$\\frac{\\partial J(w_0, w_1)}{\\partial w_0} = \\frac{1}{n}\\sum_{i=1}^n{\\frac{\\partial J(w_0, w_1)}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial w_0}} = \\frac{1}{n}\\sum_{i=1}^n{-2(y_i - \\hat{y}_i) \\times 1}$$\n",
    "#### $$\\frac{\\partial J(w_0, w_1)}{\\partial w_1} = \\frac{1}{n}\\sum_{i=1}^n{\\frac{\\partial J(w_0, w_1)}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial w_1}} = \\frac{1}{n}\\sum_{i=1}^n{-2(y_i - \\hat{y}_i) \\times x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_derivative(model, loss_func, x, y):\n",
    "    \"\"\"\n",
    "    Tensorflow is able to calculate gradient by tracking computational graph of input, therefore, we need to calculate loss so that it can calculate the derivative\n",
    "    of the loss with respect to each parameter which the input flowed through.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as g:\n",
    "        # Forward propagation\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_func(y, y_pred)\n",
    "        \n",
    "    # Calculate gradient (vector of directional derivative)\n",
    "    grads = g.gradient(loss, model.trainable_variables)\n",
    "    d_w0, d_w1 = grads[0].numpy()\n",
    "    return np.squeeze(d_w0), np.squeeze(d_w1)\n",
    "\n",
    "def derivative(model, x, y):\n",
    "    \"\"\"\n",
    "    Because we already know the loss function, we can calculate the derivative directly without having to calculate the loss first like in Tensorflow example above.\n",
    "    \"\"\"\n",
    "    # Forward propagation\n",
    "    y_pred = regression_model.predict(dummy_x)\n",
    "    \n",
    "    # Calculate derivative for each parameter\n",
    "    error = dummy_y - y_pred\n",
    "    d_w0 = np.mean(-2 * error, axis=0)\n",
    "    d_w1 = np.mean(-2 * error * dummy_x, axis=0)\n",
    "    return d_w0, d_w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow\n",
      "d_w0: -56.30768585205078\n",
      "d_w1: -4083.08349609375\n",
      "\n",
      "Calculate by hand\n",
      "d_w0: -56.307685129949824\n",
      "d_w1: -4083.083195322752\n",
      "\n",
      "Pass\n"
     ]
    }
   ],
   "source": [
    "# Initial x, y\n",
    "dummy_x = np.random.randint(0, 100, size=(10, 1))\n",
    "dummy_y = np.random.randint(0, 100, size=(10, 1))\n",
    "\n",
    "# Create model\n",
    "regression_model = LinearRegression()\n",
    "\n",
    "# Calculate derivatives using Tensorflow\n",
    "tf_d_w0, tf_d_w1 = ex_derivative(regression_model, tf.keras.losses.MeanSquaredError(), dummy_x, dummy_y)\n",
    "print(\"Using Tensorflow\")\n",
    "print(f\"d_w0: {tf_d_w0}\")\n",
    "print(f\"d_w1: {tf_d_w1}\")\n",
    "print()\n",
    "\n",
    "# Calculate derivatives by hand\n",
    "d_w0, d_w1 = derivative(regression_model, dummy_x, dummy_y)\n",
    "print(\"Calculate by hand\")\n",
    "print(f\"d_w0: {np.squeeze(d_w0)}\")\n",
    "print(f\"d_w1: {np.squeeze(d_w1[0])}\")\n",
    "print()\n",
    "\n",
    "assert abs(tf_d_w0 - d_w0) < 1e-1 and abs(tf_d_w1 - d_w1) < 1e-1\n",
    "print(\"Pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to calculate derivative, the final step to train model is to update the model parameters. The following will show you the full process of training model using **Low-level API** and **High-level API**.\n",
    "- Equations for updating each model parameter\n",
    "#### $$w_0 = w_0 - \\alpha*\\frac{\\partial J(w_0, w_1)}{\\partial w_0}$$\n",
    "#### $$w_1 = w_1 - \\alpha*\\frac{\\partial J(w_0, w_1)}{\\partial w_1}$$\n",
    "\n",
    "#### Train model using Tensorflow Low-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters before training\n",
      "offset: -0.01757599040865898\n",
      "slope: -0.03163551911711693\n",
      "\n",
      "Epoch: 1 - Loss: 17611739136.0000 - mae: 114900.8203\n",
      "Epoch: 2 - Loss: 1968597120.0000 - mae: 38414.9102\n",
      "Epoch: 3 - Loss: 220045152.0000 - mae: 12843.2617\n",
      "Epoch: 4 - Loss: 24596070.0000 - mae: 4293.8511\n",
      "Epoch: 5 - Loss: 2749265.5000 - mae: 1435.5099\n",
      "Epoch: 6 - Loss: 307306.9062 - mae: 479.8821\n",
      "Epoch: 7 - Loss: 34349.8945 - mae: 160.3852\n",
      "Epoch: 8 - Loss: 3839.3621 - mae: 53.5668\n",
      "Epoch: 9 - Loss: 429.3441 - mae: 17.8607\n",
      "Epoch: 10 - Loss: 47.9849 - mae: 5.9227\n",
      "\n",
      "Parameters after training\n",
      "offset: 0.32763591408729553 (0)\n",
      "slope: 229.99549865722656 (230)\n"
     ]
    }
   ],
   "source": [
    "# Initial x, y\n",
    "slope = 230\n",
    "offset = 0\n",
    "dummy_x = np.arange(1000).reshape(-1, 1)\n",
    "dummy_y = dummy_x * 230 + offset\n",
    "\n",
    "# Create model\n",
    "regression_model = LinearRegression()\n",
    "\n",
    "# Initial parameters\n",
    "regression_model(dummy_x)\n",
    "print(f\"Parameters before training\")\n",
    "print(f\"offset: {regression_model.trainable_variables[0].numpy()[0, 0]}\")\n",
    "print(f\"slope: {regression_model.trainable_variables[0].numpy()[1, 0]}\")\n",
    "print()\n",
    "\n",
    "# Train model\n",
    "for epoch in range(10):\n",
    "    # Calculate loss\n",
    "    with tf.GradientTape() as g:\n",
    "        y_pred = regression_model(dummy_x)\n",
    "        loss = tf.reduce_mean(tf.keras.losses.MSE(dummy_y, y_pred), axis=0)\n",
    "        \n",
    "    # Calculate gradient\n",
    "    grads = g.gradient(loss, regression_model.trainable_variables)\n",
    "    \n",
    "    # Update parameters\n",
    "    for grad, param in zip(grads, regression_model.trainable_variables):\n",
    "        param.assign(param - 1e-6 * grad)\n",
    "        \n",
    "    # Evaluate model\n",
    "    mae = tf.reduce_mean(tf.keras.metrics.MAE(dummy_y, y_pred), axis=0)\n",
    "        \n",
    "    # Plot\n",
    "    print(f\"Epoch: {epoch + 1} - Loss: {np.squeeze(loss.numpy()):.4f} - mae: {mae:.4f}\")\n",
    "print()\n",
    "    \n",
    "print(f\"Parameters after training\")\n",
    "print(f\"offset: {regression_model.trainable_variables[0].numpy()[0, 0]} ({offset})\")\n",
    "print(f\"slope: {regression_model.trainable_variables[0].numpy()[1, 0]} ({slope})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model using Tensorflow High-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters before training\n",
      "offset: -0.03309675678610802\n",
      "slope: 0.005159119609743357\n",
      "\n",
      "Train on 1000 samples\n",
      "Epoch 1/10\n",
      "1000/1000 - 0s - loss: 17606109184.0000 - mean_absolute_error: 114882.4453\n",
      "Epoch 2/10\n",
      "1000/1000 - 0s - loss: 1967968128.0000 - mean_absolute_error: 38408.7773\n",
      "Epoch 3/10\n",
      "1000/1000 - 0s - loss: 219974304.0000 - mean_absolute_error: 12841.2021\n",
      "Epoch 4/10\n",
      "1000/1000 - 0s - loss: 24588264.0000 - mean_absolute_error: 4293.1729\n",
      "Epoch 5/10\n",
      "1000/1000 - 0s - loss: 2748405.2500 - mean_absolute_error: 1435.2889\n",
      "Epoch 6/10\n",
      "1000/1000 - 0s - loss: 307214.5312 - mean_absolute_error: 479.8139\n",
      "Epoch 7/10\n",
      "1000/1000 - 0s - loss: 34338.6016 - mean_absolute_error: 160.3626\n",
      "Epoch 8/10\n",
      "1000/1000 - 0s - loss: 3838.8545 - mean_absolute_error: 53.5671\n",
      "Epoch 9/10\n",
      "1000/1000 - 0s - loss: 429.1760 - mean_absolute_error: 17.8608\n",
      "Epoch 10/10\n",
      "1000/1000 - 0s - loss: 48.0535 - mean_absolute_error: 5.9302\n",
      "\n",
      "Parameters after training\n",
      "offset: 0.31206008791923523 (0)\n",
      "slope: 229.99551391601562 (230)\n"
     ]
    }
   ],
   "source": [
    "# Initial x, y\n",
    "slope = 230\n",
    "offset = 0\n",
    "dummy_x = np.arange(1000).reshape(-1, 1)\n",
    "dummy_y = dummy_x * 230 + offset\n",
    "\n",
    "# Create model\n",
    "regression_model = LinearRegression()\n",
    "\n",
    "# Initial parameters\n",
    "regression_model(dummy_x)\n",
    "print(f\"Parameters before training\")\n",
    "print(f\"offset: {regression_model.trainable_variables[0].numpy()[0, 0]}\")\n",
    "print(f\"slope: {regression_model.trainable_variables[0].numpy()[1, 0]}\")\n",
    "print()\n",
    "\n",
    "# Compile model\n",
    "optimizer = optimizer=tf.keras.optimizers.SGD(1e-6)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metrics = tf.keras.metrics.MeanAbsoluteError()\n",
    "regression_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "# Train model\n",
    "regression_model.fit(dummy_x, dummy_y, batch_size=dummy_x.shape[0], epochs=10, verbose=2)\n",
    "print()\n",
    "\n",
    "print(f\"Parameters after training\")\n",
    "print(f\"offset: {regression_model.trainable_variables[0].numpy()[0, 0]} ({offset})\")\n",
    "print(f\"slope: {regression_model.trainable_variables[0].numpy()[1, 0]} ({slope})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1. Split validation\n",
    "\n",
    "<img src=\"pics/split_validation.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mae: 137647.38 val_mae: 164571.88\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create model\n",
    "regression_model = LinearRegression()\n",
    "\n",
    "# Compile model\n",
    "optimizer = optimizer=tf.keras.optimizers.SGD(1e-1)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metrics = tf.keras.metrics.MeanAbsoluteError()\n",
    "regression_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "# Train model\n",
    "history = regression_model.fit(train_x_scaled, train_y, batch_size=dummy_x.shape[0], epochs=10, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Show result\n",
    "train_loss = history.history[\"loss\"]\n",
    "train_mae = history.history[\"mean_absolute_error\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "val_mae = history.history[\"val_mean_absolute_error\"]\n",
    "print(f\"train_mae: {train_mae[-1]:.2f} val_mae: {val_mae[-1]:.2f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_mae)\n",
    "plt.plot(val_mae)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. K-fold cross validation\n",
    "\n",
    "<img src=\"pics/k-fold_cross_validation.png\" width=\"1100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fit model on training data and evaluate using cross validation\n",
    "class CrossValidation:\n",
    "    def __init__(self, k_folds=10, scaler=None):\n",
    "        # Initial properties\n",
    "        self.k_folds = k_folds\n",
    "        self.scaler = scaler\n",
    "        self.scores = []\n",
    "        \n",
    "    def eval(self, model, x, y, **kwargs):\n",
    "        # Initial model params\n",
    "        model(x.astype(np.float32))\n",
    "        # Save initial weights\n",
    "        model.save_weights(\"init_weights/model\")\n",
    "        \n",
    "        # Divide training set into k folds\n",
    "        kf = KFold(n_splits=self.k_folds)\n",
    "        self.scores = []\n",
    "        for i, (train_index, val_index) in enumerate(kf.split(x)):\n",
    "            # Load initial weights\n",
    "            model.load_weights(\"init_weights/model\")\n",
    "            \n",
    "            # Get validation fold\n",
    "            val_x, val_y = x[val_index], y[val_index]\n",
    "            \n",
    "            # Get training fold\n",
    "            train_x, train_y = x[train_index], y[train_index]\n",
    "            \n",
    "            # Normalization\n",
    "            if scaler is not None:\n",
    "                train_x = scaler.fit_transform(train_x)\n",
    "                val_x = scaler.transform(val_x)\n",
    "                \n",
    "            # Train model on training set\n",
    "            model.fit(train_x, train_y, **kwargs)\n",
    "            \n",
    "            # Evaluate model on validation set\n",
    "            test_loss, test_mae = regression_model.evaluate(val_x, val_y, verbose=0)\n",
    "            \n",
    "            # Save evaluation result\n",
    "            self.scores.append(test_mae)\n",
    "        # Average all evaluation results\n",
    "        mean_score = np.mean(self.scores)\n",
    "        return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation errors: [136880.75, 129183.17, 162921.73, 131706.48, 142458.66, 138309.98, 148212.67, 126787.33, 186139.55, 142249.1]\n",
      "Validation mean error: 144484.9375\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "regression_model = LinearRegression()\n",
    "# Define loss function\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "# Define metrics function\n",
    "metrics = tf.keras.metrics.MeanAbsoluteError()\n",
    "# Define optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "# Compile model\n",
    "regression_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "evaluator = CrossValidation(k_folds=10, scaler=scaler)\n",
    "\n",
    "score = evaluator.eval(regression_model, train_x, train_y, batch_size=16, epochs=10, verbose=0)\n",
    "print(f\"Validation errors: {evaluator.scores}\")\n",
    "print(f\"Validation mean error: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 132446.453125\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "regression_model = LinearRegression()\n",
    "# Define loss function\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "# Define metrics function\n",
    "metrics = tf.keras.metrics.MeanAbsoluteError()\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "# Compile model\n",
    "regression_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "history = regression_model.fit(train_x_scaled, train_y, batch_size=16, epochs=10, verbose=0)\n",
    "\n",
    "test_loss, test_mae = regression_model.evaluate(test_x_scaled, test_y, verbose=0)\n",
    "print(f\"Test error: {test_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Implement Logistic regression on qualitative data\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris_data = pd.read_csv(\"./datasets/Iris/Iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Id column\n",
    "data = iris_data.drop(columns=[\"Id\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical columns\n",
    "categorical_cols = [\"Species\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    city_encoded = pd.get_dummies(data[col])\n",
    "    city_encoded.columns = [col + \"_\" + str(_col) for _col in city_encoded.columns]\n",
    "    data = pd.concat([data.drop(columns=col), city_encoded], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate prediction/feature\n",
    "data_x = data.iloc[:, :4]\n",
    "data_y = data.iloc[:, 4:]\n",
    "print(f\"data_x: {data_x.shape}\")\n",
    "print(f\"data_y: {data_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, random_state=42, shuffle=True, stratify=data_y)\n",
    "print(f\"train_x: {train_x.shape}\")\n",
    "print(f\"test_x: {test_x.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\")\n",
    "print(f\"test_y: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Fit and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
