{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Linear Regression and Logistic Regression\n",
    "<img src=\"pics/bridge-2.jpg\" width=\"800\" height=\"400\">\n",
    "In this article you're going to learn about Linear Regression, Logistic Regression and Gradient Descent algorithm which is a essential component of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "1. How does it work?\n",
    "2. Gradient descent.\n",
    "3. Implement Linear Regression on quantitative data.\n",
    "4. Implement Logistic Regression on qualitative data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How does it work?\n",
    "Unlike KNN which needs to store whole training samples in order to predict output of unseen samples, Linear Regression and Logistic Regression do not store a single point of training sample but instead they create a linear function to approximate the training set. Below are brief procedures of how to train both of them using **Gradient Descent** (actually there is another method called **Least Square Error** but we're not going to talk about it in this course).\n",
    "```\n",
    "1. Initial linear function with random parameters\n",
    "2. for _ in range(epochs):\n",
    "3.    Predict output for training samples from the linear function\n",
    "4.    Calculate loss from prediction output and training label\n",
    "5.    Update parameters by Gradient descent\n",
    "```\n",
    "There are several differents between Linear Regression and Logistic Regression.\n",
    "1. Linear Regression outputs **continuous quantity**, Logistic Regression outputs **probability**.\n",
    "2. Linear Regression uses **Mean Squared Error** or **Mean Absolute Error** as cost function, Logistic Regression uses **Binary Cross Entropy** as cost function.\n",
    "\n",
    "In short, the obvious different between the two is that Linear Regression is for **quantitative data** and another is for **qualitative data**. the following will show how both of them work in greater detail.\n",
    "\n",
    "### 1.1. Linear Regression\n",
    "Linear Regression's equation is a simple linear function that map from N independent variables to a dependent variable.\n",
    "- Simple Linear Regression\n",
    "#### $$\\hat{y} = w_0 + wx$$\n",
    "- Multiple Linear Regression\n",
    "#### $$\\hat{y} = w_0 + w_1x_1 + w_2x_2 + w_nx_n$$\n",
    "\n",
    "<img src=\"pics/linear_regression.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "In order for Gradient Descent algorithm to be able to train the model we need a cost function for optimizer to minimize it.\n",
    "- Mean Squared Error (MSE)\n",
    "#### $$MSE = \\frac{1}{n}\\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "<img src=\"pics/MSE.png\">\n",
    "\n",
    "- Mean Absoluted Error (MAE)\n",
    "#### $$MAE = \\frac{1}{n}\\sum_{i=1}^n{|y_i - \\hat{y}_i|}$$\n",
    "\n",
    "<img src=\"pics/MAE.png\">\n",
    "\n",
    "### 1.2. Logistic Regression\n",
    "Logistic Regression is a linear equation mapping any N independent variables into one dependent variable that is a **qualitative data**.\n",
    "- Simple Logistic Regression\n",
    "#### $$\\hat{y} = \\frac{1}{1 + e^{-z}},\\quad z = w_0 + wx$$\n",
    "- Multiple Logistic Regression\n",
    "#### $$\\hat{y} = \\frac{1}{1 + e^{-z}},\\quad z = w_0 + w_1x_1 + w_2x_2 + w_nx_n$$\n",
    "\n",
    "<img src=\"pics/logistic_regression-2.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Since output of Logistic Regression is probability so it need different type of cost function.\n",
    "- Binary Cross Entropy (BCE)\n",
    "####  $$BCE = \\frac{1}{n}\\sum_{i=1}^n{(-y_i*log(\\hat{y}_i) - (1 - y_i)*log(1 - \\hat{y}_i))}$$\n",
    "\n",
    "<img src=\"pics/BCE.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient descent\n",
    "Gradient descent is an optimization algorithm used in machine learning to update model's parameters to the optimal point by minimizing some function by moving iteratively in the steepest descent direction as defined by the negative of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement Linear regression on quantitative data\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement Logistic regression on qualitative data\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
