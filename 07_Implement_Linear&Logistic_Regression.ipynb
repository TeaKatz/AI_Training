{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Linear Regression and Logistic Regression\n",
    "<img src=\"pics/bridge-2.jpg\" width=\"800\" height=\"400\">\n",
    "In this article you're going to learn about Linear Regression, Logistic Regression and Gradient Descent algorithm which is a essential component of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "1. How does it work?\n",
    "2. Gradient Descent\n",
    "3. Learning Rate\n",
    "3. Implement Linear Regression on quantitative data\n",
    "4. Implement Logistic Regression on qualitative data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How does it work?\n",
    "Unlike KNN which needs to store whole training samples in order to predict output of unseen samples, Linear Regression and Logistic Regression do not store a single point of training sample but instead they create a linear function to approximate the training set. Below are brief procedures of how to train both of them using **Gradient Descent** (actually there is another method called **Least Square Error** but we're not going to talk about it in this course).\n",
    "\n",
    "<img src=\"pics/linear_regression-2.png\" width=\"1000\">\n",
    "\n",
    "```\n",
    "1. Initial the model with random parameters\n",
    "2. for _ in range(epochs):\n",
    "3.    Predict output for training samples from the linear function\n",
    "4.    Calculate cost from prediction outputs and training labels\n",
    "5.    Update parameters by Gradient descent\n",
    "```\n",
    "\n",
    "<img src=\"pics/linear_regression_animation.gif\">\n",
    "\n",
    "There are several differents between Linear Regression and Logistic Regression.\n",
    "1. Linear Regression outputs **continuous quantity**, Logistic Regression outputs **probability**.\n",
    "2. Linear Regression uses **Mean Squared Error** or **Mean Absolute Error** as cost function, Logistic Regression uses **Binary Cross Entropy** as cost function.\n",
    "\n",
    "In short, the obvious different between the two is that Linear Regression is for **quantitative data** and another is for **qualitative data**. the following will show how both of them work in greater detail.\n",
    "\n",
    "### 1.1. Linear Regression\n",
    "Linear Regression's equation is a simple linear function that map from N independent variables to a dependent variable.\n",
    "- Simple Linear Regression\n",
    "#### $$\\hat{y} = w_0 + wx$$\n",
    "- Multiple Linear Regression\n",
    "#### $$\\hat{y} = w_0 + w_1x_1 + w_2x_2 + w_nx_n$$\n",
    "- Polynomial Linear Regression\n",
    "#### $$\\hat{y} = w_0 + w_1x_1 + w_2x_2 + w_3x_1x_2 + w_4x_1^2 + w_5x_2^2 + w_nx_n$$\n",
    "\n",
    "<img src=\"pics/linear_regression-3.png\" width=\"800\" height=\"200\">\n",
    "\n",
    "In order for Gradient Descent algorithm to be able to train the model we need a cost function for optimizer to minimize it.\n",
    "- Mean Squared Error (MSE)\n",
    "#### $$MSE = \\frac{1}{n}\\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "<img src=\"pics/MSE.png\">\n",
    "\n",
    "- Mean Absoluted Error (MAE)\n",
    "#### $$MAE = \\frac{1}{n}\\sum_{i=1}^n{|y_i - \\hat{y}_i|}$$\n",
    "\n",
    "<img src=\"pics/MAE.png\">\n",
    "\n",
    "### 1.2. Logistic Regression\n",
    "Logistic Regression is a linear equation mapping any N independent variables into one dependent variable that is a **qualitative data**.\n",
    "- Simple Logistic Regression\n",
    "#### $$\\hat{y} = \\frac{1}{1 + e^{-z}},\\quad z = w_0 + wx$$\n",
    "- Multiple Logistic Regression\n",
    "#### $$\\hat{y} = \\frac{1}{1 + e^{-z}},\\quad z = w_0 + w_1x_1 + w_2x_2 + w_nx_n$$\n",
    "- Polynomial Logistic Regression\n",
    "#### $$\\hat{y} = \\frac{1}{1 + e^{-z}},\\quad z = w_0 + w_1x_1 + w_2x_2 + w_3x_1x_2 + w_4x_1^2 + w_5x_2^2 + w_nx_n$$\n",
    "\n",
    "<img src=\"pics/logistic_regression-3.png\" width=\"1400\" height=\"200\">\n",
    "\n",
    "Since output of Logistic Regression is probability so it need different type of cost function.\n",
    "- Binary Cross Entropy (BCE)\n",
    "####  $$BCE = \\frac{1}{n}\\sum_{i=1}^n{(-y_i*log(\\hat{y}_i) - (1 - y_i)*log(1 - \\hat{y}_i))}$$\n",
    "\n",
    "<img src=\"pics/BCE.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent\n",
    "Gradient descent is an optimization algorithm used in machine learning to update model's parameters to the optimal point by minimizing some function by moving iteratively in the steepest descent direction as defined by the negative of the gradient.\n",
    "- Gradient Descent Equation\n",
    "#### $$W_{new} = W_{old} - \\alpha*\\frac{\\partial J(W)}{\\partial W}$$\n",
    "When  \n",
    "$J(W)$: Cost function  \n",
    "$W$: Model parameters  \n",
    "$\\alpha$: Learning rate\n",
    "\n",
    "<img src=\"pics/GradientDescent.png\" width=\"500\">\n",
    "\n",
    "From the figure above the model with initialized parameters start at the **inital point**, then in each epoch the model finds which direction to update each parameter to move closer to the **optimal point** by calculating partial derivative of the cost function with respect to each parameter, and update its parameters by subtraction current value of each parameter with the derivative times learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Learning Rate\n",
    "Learning rate is one of hyperparameters which used to determine how much to update parameters, too small learning rate leads to slow in training time and too big learning rate leads to oscillation or divergence.\n",
    "\n",
    "<img src=\"pics/LearningRate.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement Linear regression on quantitative data\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "house_data = pd.read_csv(\"./datasets/housedata/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns\n",
    "data = house_data.drop(columns=[\"date\", \"street\", \"country\", \"house_age\", \"statezip\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical columns\n",
    "categorical_cols = [\"view\", \"condition\", \"city\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    city_encoded = pd.get_dummies(data[col])\n",
    "    city_encoded.columns = [col + \"_\" + str(_col) for _col in city_encoded.columns]\n",
    "    data = pd.concat([data.drop(columns=col), city_encoded], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate prediction/feature\n",
    "data_x = data.drop(columns=\"price\")\n",
    "data_y = data.price\n",
    "print(f\"data_x: {data_x.shape}\")\n",
    "print(f\"data_y: {data_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "# Group y into bins\n",
    "bins = np.linspace(0, 1500000, 10)\n",
    "y_binned = np.digitize(data_y, bins)\n",
    "plt.hist(y_binned)\n",
    "\n",
    "# Split with stratify\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, random_state=42, shuffle=True, stratify=y_binned)\n",
    "print(f\"train_x: {train_x.shape}\")\n",
    "print(f\"test_x: {test_x.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\")\n",
    "print(f\"test_y: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all features to be in range [0, 1] for training set\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x)\n",
    "\n",
    "train_x_scaled = scaler.transform(train_x)\n",
    "train_x_scaled = pd.DataFrame(train_x_scaled, columns=train_x.columns)\n",
    "train_x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all features to be in range [0, 1] for test set\n",
    "test_x_scaled = scaler.transform(test_x)\n",
    "test_x_scaled = pd.DataFrame(test_x_scaled, columns=test_x.columns)\n",
    "test_x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert type to numpy array\n",
    "train_x_scaled = train_x_scaled.to_numpy()\n",
    "train_x, train_y = train_x.to_numpy(), train_y.to_numpy()\n",
    "test_x_scaled, test_y = test_x_scaled.to_numpy(), test_y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.linear_model import LinearRegression as ExampleLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        # w shape: (feature_size, 1)\n",
    "        self.w = self.add_weight(name=\"w\",\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer=tf.random_normal_initializer(),\n",
    "                                 trainable=True,\n",
    "                                 dtype=\"float32\")\n",
    "        # b shape: (1, )\n",
    "        self.b = self.add_weight(name=\"b\",\n",
    "                                 shape=(1, ),\n",
    "                                 initializer=tf.random_normal_initializer(),\n",
    "                                 trainable=True,\n",
    "                                 dtype=\"float32\")\n",
    "    \n",
    "    def call(self, inp):\n",
    "        \"\"\"\n",
    "        inp shape: (batch_size, feature_size)\n",
    "        out shape: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        out = tf.matmul(inp, self.w) + self.b\n",
    "        return out\n",
    "    \n",
    "class LinearRegression(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer = LinearLayer()\n",
    "        \n",
    "    def call(self, inp):\n",
    "        out = self.linear_layer(inp)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "regression_model = LinearRegression()\n",
    "\n",
    "x = np.random.uniform(low=0, high=5, size=(100, 50)).astype(dtype=np.float32)\n",
    "y = np.random.uniform(low=0, high=1000, size=(100, )).astype(dtype=np.float32)\n",
    "\n",
    "y_pred = regression_model.predict(x)\n",
    "\n",
    "assert y_pred.shape == (x.shape[0], 1)\n",
    "print(\"Pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e826462745dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mregression_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mex_regression_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized keyword arguments: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2872\u001b[0m     \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2873\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2874\u001b[0;31m       raise RuntimeError('You must compile your model before '\n\u001b[0m\u001b[1;32m   2875\u001b[0m                          \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2876\u001b[0m                          'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "# Test LinearRegression\n",
    "regression_model = LinearRegression()\n",
    "ex_regression_model = ExampleLinearRegression()\n",
    "\n",
    "x = np.random.uniform(low=0, high=5, size=(10, 5))\n",
    "y = np.random.uniform(low=0, high=1000, size=(10, ))\n",
    "\n",
    "regression_model.fit(x, y)\n",
    "ex_regression_model.fit(x, y)\n",
    "\n",
    "pred = regression_model.predict(x)\n",
    "ex_pred = ex_regression_model.predict(x)\n",
    "\n",
    "assert np.all(np.equal(pred, ex_pred))\n",
    "print(\"Pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Fit and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import MeanAbsoluteError\n",
    "from utilities import CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "metrics = MeanAbsoluteError()\n",
    "scaler = StandardScaler()\n",
    "evaluator = CrossValidation(metrics, k_folds=10, scaler=scaler)\n",
    "\n",
    "score = evaluator.eval(model, train_x, train_y, verbose=0)\n",
    "print(f\"Validation errors: {evaluator.scores}\")\n",
    "print(f\"Validation mean error: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Search for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Implement Logistic regression on qualitative data\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
