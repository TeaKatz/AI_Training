{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Linear Regression and Logistic Regression\n",
    "<img src=\"pics/bridge-2.jpg\" width=\"800\" height=\"400\">\n",
    "In this article you're going to learn about Linear Regression, Logistic Regression and Gradient Descent algorithm which is a essential component of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "1. How does it work?\n",
    "2. Gradient Descent\n",
    "3. Learning Rate\n",
    "3. Implement Linear Regression on quantitative data\n",
    "4. Implement Logistic Regression on qualitative data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How does it work?\n",
    "Unlike KNN which needs to store whole training samples in order to predict output of unseen samples, Linear Regression and Logistic Regression do not store a single point of training sample but instead they create a linear function to approximate the training set. Below are brief procedures of how to train both of them using **Gradient Descent** (actually there is another method called **Least Square Error** but we're not going to talk about it in this course).\n",
    "\n",
    "<img src=\"pics/linear_regression-2.png\" width=\"1000\">\n",
    "\n",
    "```\n",
    "1. Initial the model with random parameters\n",
    "2. for _ in range(epochs):\n",
    "3.    Predict output for training samples from the linear function\n",
    "4.    Calculate cost from prediction outputs and training labels\n",
    "5.    Update parameters by Gradient descent\n",
    "```\n",
    "\n",
    "<img src=\"pics/linear_regression_animation.gif\">\n",
    "\n",
    "There are several differents between Linear Regression and Logistic Regression.\n",
    "1. Linear Regression outputs **continuous quantity**, Logistic Regression outputs **probability**.\n",
    "2. Linear Regression uses **Mean Squared Error** or **Mean Absolute Error** as cost function, Logistic Regression uses **Binary Cross Entropy** as cost function.\n",
    "\n",
    "In short, the obvious different between the two is that Linear Regression is for **quantitative data** and another is for **qualitative data**. the following will show how both of them work in greater detail.\n",
    "\n",
    "### 1.1. Linear Regression\n",
    "Linear Regression's equation is a simple linear function that map from N independent variables to a dependent variable.\n",
    "- Simple Linear Regression\n",
    "#### $$\\hat{y} = w_0 + wx$$\n",
    "- Multiple Linear Regression\n",
    "#### $$\\hat{y} = w_0 + w_1x_1 + w_2x_2 + w_nx_n$$\n",
    "- Polynomial Linear Regression\n",
    "#### $$\\hat{y} = w_0 + w_1x_1 + w_2x_2 + w_3x_1x_2 + w_4x_1^2 + w_5x_2^2 + w_nx_n$$\n",
    "\n",
    "<img src=\"pics/linear_regression-3.png\" width=\"800\" height=\"200\">\n",
    "\n",
    "In order for Gradient Descent algorithm to be able to train the model we need a cost function for optimizer to minimize it.\n",
    "- Mean Squared Error (MSE)\n",
    "#### $$MSE = \\frac{1}{n}\\sum_{i=1}^n{(y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "<img src=\"pics/MSE.png\">\n",
    "\n",
    "- Mean Absolute Error (MAE)\n",
    "#### $$MAE = \\frac{1}{n}\\sum_{i=1}^n{|y_i - \\hat{y}_i|}$$\n",
    "\n",
    "<img src=\"pics/MAE.png\">\n",
    "\n",
    "### 1.2. Logistic Regression\n",
    "Logistic Regression is a linear equation mapping any N independent variables into one dependent variable that is a **qualitative data**.\n",
    "- Simple Logistic Regression\n",
    "#### $$\\hat{y} = \\frac{1}{1 + e^{-z}},\\quad z = w_0 + wx$$\n",
    "- Multiple Logistic Regression\n",
    "#### $$\\hat{y} = \\frac{1}{1 + e^{-z}},\\quad z = w_0 + w_1x_1 + w_2x_2 + w_nx_n$$\n",
    "- Polynomial Logistic Regression\n",
    "#### $$\\hat{y} = \\frac{1}{1 + e^{-z}},\\quad z = w_0 + w_1x_1 + w_2x_2 + w_3x_1x_2 + w_4x_1^2 + w_5x_2^2 + w_nx_n$$\n",
    "- Multi-classes Logistic Regression\n",
    "#### $$\\hat{y} = \\frac{e^{z_j}}{\\sum_{j=0}^{k} e^{z_j}},\\quad z_j = w_{0j} + w_jx$$\n",
    "\n",
    "<img src=\"pics/logistic_regression-3.png\" width=\"1400\" height=\"200\">\n",
    "\n",
    "Since output of Logistic Regression is probability so it need different type of cost function.\n",
    "- Binary Cross Entropy (BCE)\n",
    "####  $$BCE = \\frac{1}{n}\\sum_{i=1}^n{(-y_i*log(\\hat{y}_i) - (1 - y_i)*log(1 - \\hat{y}_i))}$$\n",
    "- Categorical Cross Entropy (CCE)\n",
    "####  $$CCE = \\frac{1}{n}\\sum_{i=1}^n{(-y_i*log(\\hat{y}_i))}$$\n",
    "\n",
    "<img src=\"pics/BCE.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gradient Descent\n",
    "Gradient descent is an optimization algorithm used in machine learning to update model's parameters to the optimal point by minimizing some function by moving iteratively in the steepest descent direction as defined by the negative of the gradient.\n",
    "- Gradient Descent Equation\n",
    "#### $$W_{new} = W_{old} - \\alpha*\\frac{\\partial J(W)}{\\partial W}$$\n",
    "When  \n",
    "$J(W)$: Cost function  \n",
    "$W$: Model parameters  \n",
    "$\\alpha$: Learning rate\n",
    "\n",
    "<img src=\"pics/GradientDescent.png\" width=\"500\">\n",
    "\n",
    "From the figure above the model with initialized parameters start at the **inital point**, then in each epoch the model finds which direction to update each parameter to move closer to the **optimal point** by calculating partial derivative of the cost function with respect to each parameter, and update its parameters by subtraction current value of each parameter with the derivative times learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Learning Rate\n",
    "Learning rate is one of hyperparameters which used to determine how much to update parameters, too small learning rate leads to slow in training time and too big learning rate leads to oscillation or divergence.\n",
    "\n",
    "<img src=\"pics/LearningRate.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement Linear regression on quantitative data\n",
    "This time we're going to use Linear Regression to predict house price instead of KNN, let see if Linear Regression can do any better than the KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "house_data = pd.read_csv(\"./datasets/housedata/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns\n",
    "data = house_data.drop(columns=[\"date\", \"yr_built\", \"yr_renovated\", \"street\", \"statezip\", \"country\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical columns\n",
    "categorical_cols = [\"view\", \"condition\", \"city\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    city_encoded = pd.get_dummies(data[col])\n",
    "    city_encoded.columns = [col + \"_\" + str(_col) for _col in city_encoded.columns]\n",
    "    data = pd.concat([data.drop(columns=col), city_encoded], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate prediction/feature\n",
    "data_x = data.drop(columns=\"price\")\n",
    "data_y = data.price\n",
    "print(f\"data_x: {data_x.shape}\")\n",
    "print(f\"data_y: {data_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "# Group y into bins\n",
    "bins = np.linspace(0, 1500000, 10)\n",
    "y_binned = np.digitize(data_y, bins)\n",
    "plt.hist(y_binned)\n",
    "\n",
    "# Split with stratify\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, random_state=42, shuffle=True, stratify=y_binned)\n",
    "print(f\"train_x: {train_x.shape}\")\n",
    "print(f\"test_x: {test_x.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\")\n",
    "print(f\"test_y: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all features to have mean of 0 and standard deviation of 1\n",
    "scaler_x = StandardScaler()\n",
    "\n",
    "scaler_x.fit(train_x)\n",
    "\n",
    "train_x_scaled = scaler_x.transform(train_x)\n",
    "train_x_scaled = pd.DataFrame(train_x_scaled, columns=train_x.columns)\n",
    "train_x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all features to have mean of 0 and standard deviation of 1\n",
    "test_x_scaled = scaler_x.transform(test_x)\n",
    "test_x_scaled = pd.DataFrame(test_x_scaled, columns=test_x.columns)\n",
    "test_x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert type to numpy array\n",
    "train_x_scaled = train_x_scaled.to_numpy()\n",
    "train_x, train_y = train_x.to_numpy(), train_y.to_numpy()\n",
    "\n",
    "test_x_scaled = test_x_scaled.to_numpy()\n",
    "test_x, test_y = test_x.to_numpy(), test_y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Prepare model\n",
    "- Implement Linear Regression in matrix form\n",
    "#### $$\\hat{Y} = XW + B$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$\\hat{Y} =\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} & x_{1m} \\\\\n",
    "x_{21} & x_{22} & x_{23} & x_{2m} \\\\\n",
    "x_{n1} & x_{n2} & x_{n3} & x_{nm} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_{11} \\\\\n",
    "w_{21} \\\\\n",
    "w_{31} \\\\\n",
    "w_{m1} \\\\\n",
    "\\end{bmatrix} + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$\\hat{Y} =\n",
    "\\begin{bmatrix}\n",
    "x_{11}w_{11} + x_{12}w_{21} + x_{13}w_{31} + x_{1m}w_{m1} \\\\\n",
    "x_{21}w_{11} + x_{22}w_{21} + x_{23}w_{31} + x_{2m}w_{m1} \\\\\n",
    "x_{n1}w_{11} + x_{n2}w_{21} + x_{n3}w_{31} + x_{nm}w_{m1} \\\\\n",
    "\\end{bmatrix} + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        # w shape: (feature_size, 1)\n",
    "        self.w = self.add_weight(name=\"W\",\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer=tf.random_normal_initializer(),\n",
    "                                 trainable=True,\n",
    "                                 dtype=\"float32\")\n",
    "        # b shape: (1, )\n",
    "        self.b = self.add_weight(name=\"B\",\n",
    "                                 shape=(1, ),\n",
    "                                 initializer=tf.random_normal_initializer(),\n",
    "                                 trainable=True,\n",
    "                                 dtype=\"float32\")\n",
    "    \n",
    "    def call(self, inp):\n",
    "        \"\"\"\n",
    "        inp shape: (batch_size, feature_size)\n",
    "        out shape: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Put your code here\n",
    "    \n",
    "class LinearRegression(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer = LinearLayer()\n",
    "        \n",
    "    def call(self, inp):\n",
    "        # Put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fit model on dummy data\n",
    "target_slope = 230\n",
    "target_bias = 1\n",
    "dummy_x = np.arange(1000).astype(np.float32).reshape(-1, 1)\n",
    "dummy_y = (dummy_x * target_slope + target_bias).astype(np.float32).reshape(-1)\n",
    "\n",
    "# Define model\n",
    "regression_model = LinearRegression()\n",
    "# Define loss function\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "# Define metrics function\n",
    "metrics = tf.keras.metrics.MeanAbsoluteError()\n",
    "# Define optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=100)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-7)\n",
    "\n",
    "# Compile model\n",
    "regression_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "# Train model\n",
    "regression_model.fit(dummy_x, dummy_y, batch_size=16, epochs=10)\n",
    "\n",
    "# Check trained parameters\n",
    "slope, bias = regression_model.trainable_variables\n",
    "print(f\"slope: {slope.numpy().squeeze()} ({target_slope})\")\n",
    "print(f\"bias: {bias.numpy().squeeze()} ({target_bias})\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(dummy_x, dummy_y, s=10)\n",
    "plt.plot(dummy_x, regression_model.predict(dummy_x), \"r\", linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Fit and evaluate model\n",
    "### 4.4.1. Split validation\n",
    "\n",
    "<img src=\"pics/split_validation.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fit model on training data and evaluate using split validation\n",
    "\n",
    "# Define model\n",
    "regression_model = LinearRegression()\n",
    "# Define loss function\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "# Define metrics function\n",
    "metrics = tf.keras.metrics.MeanAbsoluteError()\n",
    "# Define optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=600)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "# Compile model\n",
    "regression_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "# Train model\n",
    "history = regression_model.fit(train_x_scaled, train_y, batch_size=16, epochs=10, validation_split=0.2, verbose=1)\n",
    "train_loss = history.history[\"loss\"]\n",
    "train_mae = history.history[\"mean_absolute_error\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "val_mae = history.history[\"val_mean_absolute_error\"]\n",
    "print(f\"train_mae: {train_mae[-1]}\")\n",
    "print(f\"val_mae: {val_mae[-1]}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title(\"Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_mae)\n",
    "plt.plot(val_mae)\n",
    "plt.title(\"MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2. K-fold cross validation\n",
    "\n",
    "<img src=\"pics/k-fold_cross_validation.png\" width=\"1100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fit model on training data and evaluate using cross validation\n",
    "class CrossValidation:\n",
    "    def __init__(self, k_folds=10, scaler=None):\n",
    "        # Initial properties\n",
    "        self.k_folds = k_folds\n",
    "        self.scaler = scaler\n",
    "        self.scores = []\n",
    "        \n",
    "    def eval(self, model, x, y, **kwargs):\n",
    "        # Initial model params\n",
    "        model(x.astype(np.float32))\n",
    "        # Save initial weights\n",
    "        model.save_weights(\"init_weights/model\")\n",
    "        \n",
    "        # Divide training set into k folds\n",
    "        kf = KFold(n_splits=self.k_folds)\n",
    "        self.scores = []\n",
    "        for i, (train_index, val_index) in enumerate(kf.split(x)):\n",
    "            # Load initial weights\n",
    "            model.load_weights(\"init_weights/model\")\n",
    "            \n",
    "            # Get validation fold\n",
    "            val_x, val_y = x[val_index], y[val_index]\n",
    "            \n",
    "            # Get training fold\n",
    "            train_x, train_y = x[train_index], y[train_index]\n",
    "            \n",
    "            # Normalization\n",
    "            if scaler is not None:\n",
    "                train_x = scaler.fit_transform(train_x)\n",
    "                val_x = scaler.transform(val_x)\n",
    "                \n",
    "            # Train model on training set\n",
    "            model.fit(train_x, train_y, **kwargs)\n",
    "            \n",
    "            # Evaluate model on validation set\n",
    "            test_loss, test_mae = model.evaluate(val_x, val_y, verbose=0)\n",
    "            \n",
    "            # Save evaluation result\n",
    "            self.scores.append(test_mae)\n",
    "        # Average all evaluation results\n",
    "        mean_score = np.mean(self.scores)\n",
    "        return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "regression_model = LinearRegression()\n",
    "# Define loss function\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "# Define metrics function\n",
    "metrics = tf.keras.metrics.MeanAbsoluteError()\n",
    "# Define optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=600)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "# Compile model\n",
    "regression_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "evaluator = CrossValidation(k_folds=10, scaler=scaler)\n",
    "\n",
    "score = evaluator.eval(regression_model, train_x, train_y, batch_size=16, epochs=10, verbose=0)\n",
    "print(f\"Validation errors: {evaluator.scores}\")\n",
    "print(f\"Validation mean error: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "regression_model = LinearRegression()\n",
    "# Define loss function\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "# Define metrics function\n",
    "metrics = tf.keras.metrics.MeanAbsoluteError()\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "# Compile model\n",
    "regression_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "history = regression_model.fit(train_x_scaled, train_y, batch_size=16, epochs=10, verbose=0)\n",
    "\n",
    "test_loss, test_mae = regression_model.evaluate(test_x_scaled, test_y, verbose=0)\n",
    "print(f\"Test error: {test_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Implement Logistic regression on qualitative data\n",
    "Now let's using Logistic Regression to predict iris flow species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris_data = pd.read_csv(\"./datasets/Iris/Iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Id column\n",
    "data = iris_data.drop(columns=[\"Id\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical columns\n",
    "categorical_cols = [\"Species\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    city_encoded = pd.get_dummies(data[col])\n",
    "    city_encoded.columns = [col + \"_\" + str(_col) for _col in city_encoded.columns]\n",
    "    data = pd.concat([data.drop(columns=col), city_encoded], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate prediction/feature\n",
    "data_x = data.iloc[:, :4]\n",
    "data_y = data.iloc[:, 4:]\n",
    "print(f\"data_x: {data_x.shape}\")\n",
    "print(f\"data_y: {data_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, random_state=42, shuffle=True, stratify=data_y)\n",
    "print(f\"train_x: {train_x.shape}\")\n",
    "print(f\"test_x: {test_x.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\")\n",
    "print(f\"test_y: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all features to have mean of 0 and standard deviation of 1\n",
    "scaler_x = StandardScaler()\n",
    "\n",
    "scaler_x.fit(train_x)\n",
    "\n",
    "train_x_scaled = scaler_x.transform(train_x)\n",
    "train_x_scaled = pd.DataFrame(train_x_scaled, columns=train_x.columns)\n",
    "train_x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all features to have mean of 0 and standard deviation of 1\n",
    "test_x_scaled = scaler_x.transform(test_x)\n",
    "test_x_scaled = pd.DataFrame(test_x_scaled, columns=test_x.columns)\n",
    "test_x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert type to numpy array\n",
    "train_x_scaled = train_x_scaled.to_numpy()\n",
    "train_x, train_y = train_x.to_numpy(), train_y.to_numpy()\n",
    "\n",
    "test_x_scaled = test_x_scaled.to_numpy()\n",
    "test_x, test_y = test_x.to_numpy(), test_y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Prepare model\n",
    "- Implement Logistic Regression in matrix form\n",
    "#### $$\\hat{Y} = \\frac{1}{1 + e^{-Z}},\\quad Z = XW + B$$\n",
    "- Implement Multi-classes Logistic Regression in matrix form\n",
    "#### $$\\hat{Y} = \\frac{e^{Z_j}}{\\sum_{j=0}^{k} e^{Z_j}},\\quad Z = XW + B$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(layers.Layer):\n",
    "    def __init__(self, class_nums):\n",
    "        super().__init__()\n",
    "        self.class_nums = class_nums\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # w shape: (feature_size, 1)\n",
    "        self.w = self.add_weight(name=\"W\",\n",
    "                                 shape=(input_shape[-1], self.class_nums),\n",
    "                                 initializer=tf.random_normal_initializer(),\n",
    "                                 trainable=True,\n",
    "                                 dtype=\"float32\")\n",
    "        # b shape: (1, )\n",
    "        self.b = self.add_weight(name=\"B\",\n",
    "                                 shape=(self.class_nums, ),\n",
    "                                 initializer=tf.random_normal_initializer(),\n",
    "                                 trainable=True,\n",
    "                                 dtype=\"float32\")\n",
    "    \n",
    "    def call(self, inp):\n",
    "        \"\"\"\n",
    "        inp shape: (batch_size, feature_size)\n",
    "        out shape: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Put your code here\n",
    "    \n",
    "class LogisticRegression(Model):\n",
    "    def __init__(self, class_nums):\n",
    "        super().__init__()\n",
    "        self.class_nums = class_nums\n",
    "        self.linear_layer = LinearLayer(class_nums)\n",
    "        \n",
    "    def _sigmoid(self, z):\n",
    "        # Put your code here\n",
    "    \n",
    "    def _softmax(self, z):\n",
    "        # Put your code here\n",
    "        \n",
    "    def call(self, inp):\n",
    "        # Put your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fit model on dummy data\n",
    "dummy_x = np.arange(-500, 500).astype(np.float32).reshape(-1, 1)\n",
    "dummy_y = (dummy_x > 0).astype(np.float32).reshape(-1)\n",
    "\n",
    "# Define model\n",
    "classifier_model = LogisticRegression(class_nums=1)\n",
    "# Define loss function\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "# Define metrics function\n",
    "metrics = tf.keras.metrics.BinaryAccuracy(threshold=0.5)\n",
    "# Define optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
    "\n",
    "# Compile model\n",
    "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "# Train model\n",
    "classifier_model.fit(dummy_x, dummy_y, batch_size=16, epochs=10)\n",
    "\n",
    "# Check trained parameters\n",
    "w, b = classifier_model.trainable_variables\n",
    "print(f\"w: {w.numpy()}\")\n",
    "print(f\"b: {b.numpy()}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(dummy_x, dummy_y, s=10)\n",
    "plt.plot(dummy_x, classifier_model.predict(dummy_x), \"r\", linewidth=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Fit and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "classifier_model = LogisticRegression(class_nums=3)\n",
    "# Define loss function\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "# Define metrics function\n",
    "metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "# Define optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=1)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1)\n",
    "\n",
    "# Compile model\n",
    "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "evaluator = CrossValidation(k_folds=10, scaler=scaler)\n",
    "\n",
    "score = evaluator.eval(classifier_model, train_x, train_y, batch_size=16, epochs=10, verbose=0)\n",
    "print(f\"Validation errors: {evaluator.scores}\")\n",
    "print(f\"Validation mean error: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "classifier_model = LogisticRegression(class_nums=3)\n",
    "# Define loss function\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "# Define metrics function\n",
    "metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1)\n",
    "\n",
    "# Compile model\n",
    "classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "\n",
    "history = classifier_model.fit(train_x_scaled, train_y, batch_size=16, epochs=10, verbose=0)\n",
    "\n",
    "test_loss, test_acc = classifier_model.evaluate(test_x_scaled, test_y, verbose=0)\n",
    "print(f\"Test error: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
