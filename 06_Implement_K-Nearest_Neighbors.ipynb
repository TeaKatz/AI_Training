{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement K-Nearest Neighbors\n",
    "<img src=\"pics/city-1.jpg\" width=\"800\" height=\"400\">\n",
    "In this article you're going to learn about K-Nearest Neighbors and machine learning workflow (after getting cleaned dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "1. How does it work?\n",
    "2. Calculating distance.\n",
    "3. Implement K-Nearest Neighbors on quantitative data.\n",
    "4. Implement K-Nearest Neighbors on qualitative data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How does it work?\n",
    "As the core concept of K-Nearest Neighbors is to take the majority vote of **K** samples which are the **most similar** to the new sample.  \n",
    "The procedure below show how the algorithm works.\n",
    "\n",
    "<img src=\"pics/knn-1.png\" width=\"1000\">\n",
    "\n",
    "1. **Calculate distance** between new sample and other labeled samples.\n",
    "2. **Sort** samples by its distance value from low to high.\n",
    "3. **Get first K samples** which have the lowest distance value.  \n",
    "4. \n",
    "a) If target prediction is **quantitative data**, target prediction can be calculated by **averaging** outputs of the K samples.  \n",
    "b) If target prediction is **qualitative data**, target prediction can be calculated by **getting most frequent** outputs of the K samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculating distance\n",
    "There are two ways to calculate distance for K-Nearest Neighbors.  \n",
    "\n",
    "<img src=\"pics/calculate_distance.png\" width=\"650\">\n",
    "\n",
    "2.1. Manhattan distance can be calculated as folowing:  \n",
    "#### $$d = \\sum_{i=1}^{N} |x_i - y_i|$$\n",
    "2.2. Euclidean distance can be calculated as following:\n",
    "#### $$d = \\sqrt{\\sum_{i=1}^{N} (x_i - y_i)^2}$$\n",
    "Let's implement modules to calculate Manhattan distance and Euclidean distance below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utilities.ManhattanDistance import ManhattanDistance as ExampleManhattanDistance\n",
    "from utilities.EuclideanDistance import EuclideanDistance as ExampleEuclideanDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManhattanDistance:\n",
    "    def __call__(self, samples, new_sample):\n",
    "        \"\"\" \n",
    "        samples shape: (sample_nums, feature_nums)\n",
    "        new_sample shape: (1, feature_nums)\n",
    "        Return Manhanttan distance between new sample point and other sample points with shape (sample_nums, )\n",
    "        \"\"\"\n",
    "        # Put your code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ManhattanDistance\n",
    "samples = np.random.uniform(-10, 10, size=(100, 10))\n",
    "new_sample = np.random.uniform(-10, 10, size=(1, 10))\n",
    "\n",
    "# Test Manhanttan Distance\n",
    "example_manhanttan = ExampleManhattanDistance()\n",
    "manhanttan = ManhattanDistance()\n",
    "\n",
    "example_manhanttan_distance = example_manhanttan(samples, new_sample)\n",
    "manhanttan_distance = manhanttan(samples, new_sample)\n",
    "\n",
    "assert np.sum(example_manhanttan_distance - manhanttan_distance, dtype=np.float32) == 0.0\n",
    "print(\"pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuclideanDistance:\n",
    "    def __call__(self, samples, new_sample):\n",
    "        \"\"\"\n",
    "        samples shape: (sample_nums, feature_nums)\n",
    "        new_sample shape: (1, feature_nums)\n",
    "        Return Euclidean distance between new sample point and other sample points with shape (sample_nums, )\n",
    "        \"\"\"\n",
    "        # Put your code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test EuclideanDistance\n",
    "samples = np.random.uniform(-10, 10, size=(100, 10))\n",
    "new_sample = np.random.uniform(-10, 10, size=(1, 10))\n",
    "\n",
    "# Test Euclidean Distance\n",
    "example_euclidean = ExampleEuclideanDistance()\n",
    "euclidean = EuclideanDistance()\n",
    "\n",
    "example_euclidean_distance = example_euclidean(samples, new_sample)\n",
    "euclidean_distance = euclidean(samples, new_sample)\n",
    "\n",
    "assert np.sum(example_euclidean_distance - euclidean_distance, dtype=np.float32) == 0.0\n",
    "print(\"pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement K-Nearest Neighbors on quantitative data\n",
    "In this practice we're going to use KNN to predict house price. The dataset has 4600 samples each consists of price that we want to predict and various features such as number of bedrooms, number of bathrooms, number of floors, size, location and etc., we need to preprocess such features and use them to predict the particular house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "house_data = pd.read_csv(\"./datasets/housedata/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some samples of the dataset\n",
    "house_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 18 columns, 1 columns for target price prediction and and 17 columns for features. However, we do not need to use all of them so let's investigate which one should we cut it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date\n",
    "house_data.date.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date column is almost useless **in this case** because all sample gathered in small time period (about 2 months) in 2014 so that inflation is not to be considered in this dataset, but keep in mind that model learns only from this dataset is not going to work well when predict nowadays house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition, sqft_above, sqft_basement, yr_built, yr_renovated\n",
    "cols = [\"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\", \"floors\", \"waterfront\", \"view\", \"condition\", \"sqft_above\", \"sqft_basement\", \"yr_built\", \"yr_renovated\"]\n",
    "plt.figure(figsize=(24, 12))\n",
    "\n",
    "for i, column_name in enumerate(cols):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.scatter(house_data[column_name], house_data.price)\n",
    "    plt.title(column_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 3 outlier points that make all graph look flat, so let's remove that outlier points and plot the graph again.  \n",
    "Moreover, yr_built and yr_renovated are repeatitive so let's convert them into house_age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert yr_built and yr_renovated into house_age\n",
    "house_age = 2014 - np.maximum(house_data.yr_built, house_data.yr_renovated)\n",
    "house_age = pd.Series(house_age, name=\"house_age\")\n",
    "house_data = pd.concat([house_data.drop(columns=[\"yr_built\", \"yr_renovated\"]), house_age], axis=1)\n",
    "\n",
    "# Update cols\n",
    "cols.remove(\"yr_built\")\n",
    "cols.remove(\"yr_renovated\")\n",
    "cols.append(\"house_age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition, sqft_above, sqft_basement, house_age\n",
    "plt.figure(figsize=(24, 12))\n",
    "\n",
    "without_outlier = house_data.query(\"0 < price < 5000000\")\n",
    "for i, column_name in enumerate(cols):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.scatter(without_outlier[column_name], without_outlier.price)\n",
    "    plt.title(column_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, all 12 features above look useful except house_age that almost competely flat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# street, city, statezip, country\n",
    "print(f\"street unique count: {len(house_data.street.unique())}\")\n",
    "print(f\"city unique count: {len(house_data.city.unique())}\")\n",
    "print(f\"statezip unique count: {len(house_data.statezip.unique())}\")\n",
    "print(f\"country unique count: {len(house_data.country.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these features are telling us about the same thing, that is location. Actually it would be better if we can turn these features into longitude and latitude, but for now let's use it this way.  \n",
    "**street** is not really useful in this case because it almost specific to each of the sample.  \n",
    "**city** and **statezip** can be useful but they are repetitive because both are location information, however, statezip is more precise than city. For this reason, only use one of them would be find.  \n",
    "**country** is completely useless in this case because it identical to every samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, \n",
    "1. We will not utilize the following columns **date**, **street**, **country**, **house_age** and either **city** or **statezip**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns\n",
    "data = house_data.drop(columns=[\"date\", \"street\", \"country\", \"house_age\", \"statezip\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical columns\n",
    "categorical_cols = [\"view\", \"condition\", \"city\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    city_encoded = pd.get_dummies(data[col])\n",
    "    city_encoded.columns = [col + \"_\" + str(_col) for _col in city_encoded.columns]\n",
    "    data = pd.concat([data.drop(columns=col), city_encoded], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate prediction/feature\n",
    "data_x = data.drop(columns=\"price\")\n",
    "data_y = data.price\n",
    "print(f\"data_x: {data_x.shape}\")\n",
    "print(f\"data_y: {data_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "# Group y into bins\n",
    "bins = np.linspace(0, 1500000, 10)\n",
    "y_binned = np.digitize(data_y, bins)\n",
    "plt.hist(y_binned)\n",
    "\n",
    "# Split with stratify\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, random_state=42, shuffle=True, stratify=y_binned)\n",
    "print(f\"train_x: {train_x.shape}\")\n",
    "print(f\"test_x: {test_x.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\")\n",
    "print(f\"test_y: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all features to be in range [0, 1] for training set\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x)\n",
    "\n",
    "train_x_scaled = scaler.transform(train_x)\n",
    "train_x_scaled = pd.DataFrame(train_x_scaled, columns=train_x.columns)\n",
    "train_x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all features to be in range [0, 1] for test set\n",
    "test_x_scaled = scaler.transform(test_x)\n",
    "test_x_scaled = pd.DataFrame(test_x_scaled, columns=test_x.columns)\n",
    "test_x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert type to numpy array\n",
    "train_x_scaled = train_x_scaled.to_numpy()\n",
    "train_x, train_y = train_x.to_numpy(), train_y.to_numpy()\n",
    "test_x_scaled, test_y = test_x_scaled.to_numpy(), test_y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionKNN:\n",
    "    def __init__(self, k=5, distance_method=\"Euclidean\"):\n",
    "        # Put your code here\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        # Put your code here\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Put your code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RegressionKNN\n",
    "regression_model = RegressionKNN(k=5, distance_method=\"Euclidean\")    # KNeighborsRegressor uses Euclidean distance\n",
    "ex_regression_model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "x = np.random.uniform(low=0, high=5, size=(10, 5))\n",
    "y = np.random.uniform(low=0, high=1000, size=(10, ))\n",
    "\n",
    "regression_model.fit(x, y)\n",
    "ex_regression_model.fit(x, y)\n",
    "\n",
    "pred = regression_model.predict(x)\n",
    "ex_pred = ex_regression_model.predict(x)\n",
    "\n",
    "assert np.all(np.equal(pred, ex_pred))\n",
    "print(\"Pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Fit and evaluate model\n",
    "Before start training our model we must decide\n",
    "### 3.5.1. Evaluate metrics\n",
    "Metrics are functions that is used to measure how good or bad is our model. One of the nost popular metrics for quantitative data is **Mean Absolute Error (MAE)**, it is used to measure the average magnitude of errors between predictions and actual observations by calculating mean absolute differences between the two values.\n",
    "#### $$MAE = \\frac{1}{n}\\sum_{i=1}^{N} |y_i - \\hat{y_i}|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import MeanAbsoluteError as ExampleMeanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAbsoluteError:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true: Target predictions with shape (batch_size, class_nums)\n",
    "        y_pred: Predictions with shape (batch_size, class_nums)\n",
    "        Return scalar value of MAE\n",
    "        \"\"\"\n",
    "        # Put your code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MeanAbsoluteError\n",
    "y_true = np.random.uniform(low=0, high=5, size=(10, 5))\n",
    "y_pred = np.random.uniform(low=0, high=5, size=(10, 5))\n",
    "\n",
    "metrics_example = ExampleMeanAbsoluteError()\n",
    "metrics = MeanAbsoluteError()\n",
    "\n",
    "mae_example = metrics_example(y_true, y_pred)\n",
    "mae = metrics_example(y_true, y_pred)\n",
    "print(f\"mae_example: {mae_example}\")\n",
    "print(f\"mae: {mae}\")\n",
    "\n",
    "assert mae_example == mae\n",
    "print(\"Pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2. Validation set\n",
    "Validation set is a set of data which split from training set for evaluating model while doing hyperparameter tuning to avoid data leakage.\n",
    "### 3.5.2.1. Split validation\n",
    "Split some partial of training set and use it to evaluate trained model.\n",
    "\n",
    "<img src=\"pics/split_validation.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitValidation:\n",
    "    def __init__(self, metrics, val_size=0.2, random_state=None, shuffle=True, stratify=None):\n",
    "        # Initial properties\n",
    "        self.metrics = metrics\n",
    "        self.val_size = val_size\n",
    "        self.random_state = random_state\n",
    "        self.shuffle = shuffle\n",
    "        self.stratify = stratify\n",
    "        \n",
    "    def eval(self, model, x, y):\n",
    "        # Split train/val\n",
    "        train_x, val_x, train_y, val_y = train_test_split(x, y, test_size=self.val_size, \n",
    "                                                          random_state=self.random_state, \n",
    "                                                          shuffle=self.shuffle, \n",
    "                                                          stratify=self.stratify)\n",
    "        \n",
    "        # Normalization\n",
    "        scaler = MinMaxScaler()\n",
    "        train_x = scaler.fit_transform(train_x)\n",
    "        val_x = scaler.transform(val_x)\n",
    "        \n",
    "        # Fit training set to model\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        # Evaluate model with validation set\n",
    "        pred_y = model.predict(val_x)\n",
    "        score = self.metrics(val_y.reshape(-1, 1), pred_y.reshape(-1, 1))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group y into bins\n",
    "bins = np.linspace(0, 1500000, 10)\n",
    "y_binned = np.digitize(train_y, bins)\n",
    "\n",
    "model = RegressionKNN(k=5, distance_method=\"Euclidean\")\n",
    "metrics = MeanAbsoluteError()\n",
    "evaluator = SplitValidation(metrics, random_state=42, stratify=y_binned)\n",
    "\n",
    "score = evaluator.eval(model, train_x, train_y)\n",
    "print(f\"Validation error: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage and disadvantage of using Split validation.  \n",
    "**Advantage**  \n",
    "- Cheap to compute. \n",
    "\n",
    "**Disadvantage**  \n",
    "- It sacrifice partial of training set.\n",
    "- Validation set can be bias especially when it has small size (< 10000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2.2. K-fold cross validation\n",
    "K-fold cross validation is a better way to evaluate model than the split validation by evaluating model k times each time with different trining and validation set, the following steps show how is works.  \n",
    "\n",
    "<img src=\"pics/k-fold_cross_validation.png\" width=\"1100\">\n",
    "\n",
    "```\n",
    "1. Divide training set into k folds\n",
    "2. for i in range(k):\n",
    "3.    Initial new model\n",
    "4.    validation fold is the data at fold i\n",
    "5.    training fold is the rest of the data\n",
    "6.    Train model on training_fold\n",
    "7.    Evaluate model on validation_fold\n",
    "8.    Save evaluation result\n",
    "9. Average all evaluation results\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossValidation:\n",
    "    def __init__(self, metrics, k_folds=10):\n",
    "        # Initial properties\n",
    "        self.metrics = metrics\n",
    "        self.k_folds = k_folds\n",
    "        self.scores = []\n",
    "        \n",
    "    def eval(self, model, x, y):\n",
    "        # Divide training set into k folds\n",
    "        kf = KFold(n_splits=self.k_folds)\n",
    "        self.scores = []\n",
    "        for i, (train_index, val_index) in enumerate(kf.split(x)):\n",
    "            # Get validation fold\n",
    "            val_x, val_y = x[val_index], y[val_index]\n",
    "\n",
    "            # Get training fold\n",
    "            train_x, train_y = x[train_index], y[train_index]\n",
    "\n",
    "            # Normalization\n",
    "            scaler = MinMaxScaler()\n",
    "            train_x = scaler.fit_transform(train_x)\n",
    "            val_x = scaler.transform(val_x)\n",
    "\n",
    "            # Train model on training set\n",
    "            model.fit(train_x, train_y)\n",
    "\n",
    "            # Evaluate model on validation set\n",
    "            pred_y = model.predict(val_x)\n",
    "            score = self.metrics(val_y.reshape(-1, 1), pred_y.reshape(-1, 1))\n",
    "\n",
    "            # Save evaluation result\n",
    "            self.scores.append(score)\n",
    "        # Average all evaluation results\n",
    "        mean_score = np.mean(self.scores)\n",
    "        return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionKNN(k=5, distance_method=\"Euclidean\")\n",
    "metrics = MeanAbsoluteError()\n",
    "evaluator = CrossValidation(metrics, k_folds=10)\n",
    "\n",
    "score = evaluator.eval(model, train_x, train_y)\n",
    "print(f\"Validation errors: {evaluator.scores}\")\n",
    "print(f\"Validation mean error: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage and disadvantage of using Cross validation.  \n",
    "**Advantage**  \n",
    "- Always more accurate than split validation.\n",
    "\n",
    "**Disadvantage**  \n",
    "- Computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split validation and cross validation summarize\n",
    "1. If dataset size is small (< 10000) -> cross validation.\n",
    "2. If it doesn't take too long to train and validate per model -> cross validation.\n",
    "3. otherwise -> split validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Search for best hyperparameters\n",
    "There are various of ways to search for best hyperparameters. However, there are two basic ways to do so that are Grid search and Random search.\n",
    "### 3.6.1. Grid searching\n",
    "Implementation of grid search is simple, we define a set of values for each hyperparameter then we evaluate models which created based on every combination of all set of hyperparameters and choose only the best one.\n",
    "\n",
    "<img src=\"pics/GridSearch.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearcher:\n",
    "    def __init__(self, evaluator, criteria=\"min\"):\n",
    "        assert criteria.lower() == \"min\" or criteria.lower() == \"max\", \"criteria can be either 'max' or 'min'\"\n",
    "        # Initial properties\n",
    "        self.evaluator = evaluator\n",
    "        self.criteria = criteria\n",
    "        \n",
    "    def search(self, model_class, x, y, params_grid, report=True):\n",
    "        # Get parameters space\n",
    "        param_space = list(ParameterGrid(params_grid))\n",
    "        \n",
    "        # Reset\n",
    "        best_params = None\n",
    "        best_score = None\n",
    "        for params in param_space:\n",
    "            # Create model with new parameters\n",
    "            model = model_class(**params)\n",
    "            \n",
    "            # Evaluate model\n",
    "            score = self.evaluator.eval(model, x, y)\n",
    "            \n",
    "            # Save best parameters and best score\n",
    "            if best_params is None and best_score is None:\n",
    "                best_params = params\n",
    "                best_score = score\n",
    "            else:\n",
    "                if self.criteria == \"min\":\n",
    "                    if score < best_score:\n",
    "                        best_params = params\n",
    "                        best_score = score\n",
    "                else:\n",
    "                    if score > best_score:\n",
    "                        best_params = params\n",
    "                        best_score = score\n",
    "                    \n",
    "            # Report\n",
    "            if report:\n",
    "                print(f\"parameters: {params} score: {score}\")\n",
    "        return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = MeanAbsoluteError()\n",
    "evaluator = CrossValidation(metrics, k_folds=10)\n",
    "searcher = GridSearcher(evaluator, criteria=\"min\")\n",
    "\n",
    "# Search for best parameters\n",
    "params_grid = {\"k\": list(range(1, 41)), \"distance_method\": [\"Euclidean\", \"Manhattan\"]}\n",
    "best_params, score = searcher.search(RegressionKNN, train_x, train_y, params_grid)\n",
    "print(f\"best parameters: {best_params}\")\n",
    "print(f\"score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2. Random searching\n",
    "Grid searching however has big problem when number of hyperparameters get large, trying out every combination of them take long time to run. For this reason, instead of trying out all combination we randomly try out only some of them, this way not only help **reduce search time** but also **increase number of search point for individual hyperparameter** as shown in figure below. Therefore, **Random searching is recommended** over Grid searching when number of hyperparameters is large or some hyperparameters are float.\n",
    "\n",
    "<img src=\"pics/RandomSearch.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSearcher:\n",
    "    def __init__(self, evaluator, criteria=\"min\", nums=None):\n",
    "        assert criteria.lower() == \"min\" or criteria.lower() == \"max\", \"criteria can be either 'max' or 'min'\"\n",
    "        # Initial properties\n",
    "        self.evaluator = evaluator\n",
    "        self.criteria = criteria\n",
    "        self.nums = nums\n",
    "        \n",
    "    def search(self, model_class, x, y, params_grid, report=True):\n",
    "        # Get parameters space\n",
    "        param_space = list(ParameterGrid(params_grid))\n",
    "        nums = len(param_space) if self.nums is None else self.nums\n",
    "        assert nums <= len(param_space), f\"nums cannot be bigger than {len(param_space)}\"\n",
    "        param_space = np.random.choice(param_space, size=nums, replace=False)\n",
    "        \n",
    "        # Reset\n",
    "        best_params = None\n",
    "        best_score = None\n",
    "        for params in param_space:\n",
    "            # Create model with new parameters\n",
    "            model = model_class(**params)\n",
    "            \n",
    "            # Evaluate model\n",
    "            score = self.evaluator.eval(model, x, y)\n",
    "            \n",
    "            # Save best parameters and best score\n",
    "            if best_params is None and best_score is None:\n",
    "                best_params = params\n",
    "                best_score = score\n",
    "            else:\n",
    "                if self.criteria == \"min\":\n",
    "                    if score < best_score:\n",
    "                        best_params = params\n",
    "                        best_score = score\n",
    "                else:\n",
    "                    if score > best_score:\n",
    "                        best_params = params\n",
    "                        best_score = score\n",
    "                    \n",
    "            # Report\n",
    "            if report:\n",
    "                print(f\"parameters: {params} score: {score}\")\n",
    "        return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MeanAbsoluteError()\n",
    "evaluator = CrossValidation(metrics, k_folds=10)\n",
    "searcher = RandomSearcher(evaluator, criteria=\"min\", nums=10)\n",
    "\n",
    "# Search for best parameters\n",
    "params_grid = {\"k\": list(range(1, 41)), \"distance_method\": [\"Euclidean\", \"Manhattan\"]}\n",
    "best_params, score = searcher.search(RegressionKNN, train_x, train_y, params_grid)\n",
    "print(f\"best parameters: {best_params}\")\n",
    "print(f\"score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and metrics\n",
    "model = RegressionKNN(**best_params)\n",
    "metrics = MeanAbsoluteError()\n",
    "\n",
    "# Fit model on traning set\n",
    "model.fit(train_x_scaled, train_y)\n",
    "\n",
    "# Evaluate on test set\n",
    "pred_y = model.predict(test_x_scaled)\n",
    "score = metrics(test_y.reshape(-1, 1), pred_y.reshape(-1, 1))\n",
    "print(f\"Test error: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implement K-Nearest Neighbors on qualitative data\n",
    "In this practice we're going to use KNN to classify species of iris flower. The only different here is that now our target prediction is categorical instead of continuous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris_data = pd.read_csv(\"./datasets/Iris/iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some samples of the dataset\n",
    "iris_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have very few number of samples, 150 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm\n",
    "cols = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]\n",
    "plt.figure(figsize=(24, 6))\n",
    "\n",
    "for i, column_name in enumerate(cols):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.scatter(iris_data[column_name], iris_data.Species)\n",
    "    plt.title(column_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the charts above, there are 3 categories for target prediction and 4 features which can be used to train model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Id column\n",
    "data = iris_data.drop(columns=[\"Id\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical columns\n",
    "categorical_cols = [\"Species\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    city_encoded = pd.get_dummies(data[col])\n",
    "    city_encoded.columns = [col + \"_\" + str(_col) for _col in city_encoded.columns]\n",
    "    data = pd.concat([data.drop(columns=col), city_encoded], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate prediction/feature\n",
    "data_x = data.iloc[:, :4]\n",
    "data_y = data.iloc[:, 4:]\n",
    "print(f\"data_x: {data_x.shape}\")\n",
    "print(f\"data_y: {data_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/test\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, random_state=42, shuffle=True, stratify=data_y)\n",
    "print(f\"train_x: {train_x.shape}\")\n",
    "print(f\"test_x: {test_x.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\")\n",
    "print(f\"test_y: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all features to be in range [0, 1] for training set\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_x)\n",
    "\n",
    "train_x_scaled = scaler.transform(train_x)\n",
    "train_x_scaled = pd.DataFrame(train_x_scaled, columns=train_x.columns)\n",
    "train_x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all features to be in range [0, 1] for test set\n",
    "test_x_scaled = scaler.transform(test_x)\n",
    "test_x_scaled = pd.DataFrame(test_x_scaled, columns=test_x.columns)\n",
    "test_x_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert type to numpy array\n",
    "train_x_scaled = train_x_scaled.to_numpy()\n",
    "train_x, train_y = train_x.to_numpy(), train_y.to_numpy()\n",
    "test_x_scaled, test_y = test_x_scaled.to_numpy(), test_y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Prepare model\n",
    "KNN for qualitative data works exactly the same as KNN for quatitative data but instead of averaging k closest samples it takes majority vote to get final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierKNN:\n",
    "    def __init__(self, k=5, distance_method=\"Euclidean\"):\n",
    "        # Put your code here\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        # Put your code here\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Put your code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RegressionKNN\n",
    "classifier_model = ClassifierKNN(k=5, distance_method=\"Euclidean\")    # KNeighborsClassifier uses Euclidean distance\n",
    "ex_classifier_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "x = train_x_scaled[:10]\n",
    "y = train_y[:10]\n",
    "\n",
    "classifier_model.fit(x, y)\n",
    "ex_classifier_model.fit(x, y)\n",
    "\n",
    "pred = classifier_model.predict(x)\n",
    "ex_pred = ex_classifier_model.predict(x)\n",
    "\n",
    "assert np.all(np.equal(pred, ex_pred))\n",
    "print(\"Pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Fit and evaluate model\n",
    "For qualitative data we usually use accuracy metrics to evaluate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassifierKNN(k=5, distance_method=\"Euclidean\")\n",
    "evaluator = CrossValidation(accuracy_score, k_folds=10)\n",
    "\n",
    "score = evaluator.eval(model, train_x, train_y)\n",
    "print(f\"Validation accuracy: {evaluator.scores}\")\n",
    "print(f\"Validation mean accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Search for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator = CrossValidation(accuracy_score, k_folds=10)\n",
    "searcher = GridSearcher(evaluator, criteria=\"max\")\n",
    "\n",
    "# Search for best parameters\n",
    "params_grid = {\"k\": list(range(1, 41)), \"distance_method\": [\"Euclidean\", \"Manhattan\"]}\n",
    "best_params, score = searcher.search(ClassifierKNN, train_x, train_y, params_grid)\n",
    "print(f\"best parameters: {best_params}\")\n",
    "print(f\"score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and metrics\n",
    "model = ClassifierKNN(**best_params)\n",
    "\n",
    "# Fit model on traning set\n",
    "model.fit(train_x_scaled, train_y)\n",
    "\n",
    "# Evaluate on test set\n",
    "pred_y = model.predict(test_x_scaled)\n",
    "score = accuracy_score(test_y.reshape(-1, 1), pred_y.reshape(-1, 1))\n",
    "print(f\"Test error: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
